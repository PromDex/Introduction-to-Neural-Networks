{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7429d2d-bfdc-43ae-bf9f-dd1d08ce7ebc",
   "metadata": {},
   "source": [
    "## Домашнее задание <a class='anchor' id='hw'>\n",
    "\n",
    "1. Сделайте краткий обзор любой научной работы, посвящённой алгоритму для object detection, не рассматривавшемуся на уроке.\\\n",
    "    Проведите анализ: чем отличается выбранная вами архитектура нейронной сети от других?\\\n",
    "    В чём плюсы и минусы данной архитектуры?\\\n",
    "    Какие могут возникнуть трудности при применении этой архитектуры на практике?\n",
    "\n",
    "2. Ссылка на репозиторий с полным кодом для обучения ssd нейросети: https://github.com/sergeyveneckiy/ssd-tensorflow.\\\n",
    "    Попробуйте улучшить точность её работы и напишите отчёт (что вы пробовали изменить в её параметрах и как это отражалось на процессе обучения нейронной сети)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27598d68-e4ef-4f13-8480-3c73f1f968e5",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "ПЕРВОЕ МНЕНИЕ\n",
    "\n",
    "\n",
    "- **Алгоритмы для обнаружения объектов (Object Detection) можно классифицировать на несколько категорий в зависимости от их подходов и особенностей.\\\n",
    "    Вот краткое перечисление и классификация алгоритмов для object detection:**\n",
    "\n",
    "1. *Методы на основе классификации окон **(Sliding Window-Based Methods)** :\\\n",
    "   Эти методы основаны на сканировании изображения с помощью окна фиксированного размера, которое перемещается по всему изображению с определенным шагом.\\\n",
    "   Для каждого окна применяется классификатор, чтобы определить, присутствует ли объект внутри окна. Примеры таких методов включают метод Виолы-Джонса (Viola-Jones) и метод Хогга (HOG).*\n",
    "\n",
    "\n",
    "2. *Методы на основе пирамид **(Pyramid-Based Methods)** :\\\n",
    "   Эти методы используют множество изображений с разным разрешением, создавая так называемую пирамиду изображений. Затем алгоритм применяется к каждому изображению в пирамиде, чтобы обнаружить объекты разных размеров.\\\n",
    "   Это позволяет обнаруживать объекты на разных масштабах. Примером является метод быстрой свертки     (Fast Convolutional Pyramid).*\n",
    "\n",
    "\n",
    "3. *Методы на основе обучения с учителем **(Supervised Learning-Based Methods)** :\\\n",
    "   Эти методы требуют большого набора размеченных данных для обучения.\\\n",
    "   Они используют глубокие нейронные сети, такие как SSD (Single Shot Multibox Detector) или YOLO (You Only Look Once), для одновременного обнаружения объектов и определения их классов.*\n",
    "\n",
    "\n",
    "4. *Методы на основе обучения без учителя **(Unsupervised Learning-Based Methods)** :\\\n",
    "   Эти методы пытаются обнаружить объекты без предварительной разметки данных.\\\n",
    "   Они используют техники кластеризации, выделения основных точек и алгоритмы генеративных моделей, чтобы выявлять объекты на изображениях.*\n",
    "\n",
    "\n",
    "5. *Методы на основе регионов **(Region-Based Methods)** :\\\n",
    "   Эти методы разделяют задачу обнаружения объектов на два этапа: сначала генерируются кандидаты на области, которые могут содержать объекты (регионы), а затем применяются классификаторы или регрессоры для определения наличия объекта и его точных координат.\n",
    "   Примеры методов на основе регионов включают метод Selective Search и метод EdgeBoxes.*\n",
    "\n",
    "\n",
    "6. *Методы на основе глубокого обучения **(Deep Learning-Based Methods)** :\\\n",
    "   Эти методы сосредоточены на использовании глубоких нейронных сетей, таких как сверточные нейронные сети (CNN) и рекуррентные нейронные сети (RNN), для обнаружения объектов.\\\n",
    "   Они позволяют автоматически извлекать признаки из изображений и применять их для задачи обнаружения объектов.\\\n",
    "   Некоторые популярные глубокие    методы включают Faster R-CNN, RetinaNet, SSD (Single Shot Multibox Detector) и YOLO (You Only Look Once).*\n",
    "\n",
    "\n",
    "7. *Методы на основе многомасштабных анализов **(Multi-Scale Analysis Methods)** :\\\n",
    "   Эти методы стремятся обнаруживать объекты на разных масштабах изображений, что позволяет справляться с объектами разных размеров.\\\n",
    "   Они могут использовать пирамиды изображений, различные окна и фильтры для анализа множества масштабов.\\\n",
    "   Такие подходы позволяют более эффективно обнаруживать объекты различных размеров на одном изображении.*\n",
    "\n",
    "\n",
    "8. *Методы на основе меток или ключевых точек **(Keypoint or Landmark-Based Methods)** :\\\n",
    "   Эти методы обнаруживают объекты, исходя из ключевых точек или меток, которые характеризуют объект.\\\n",
    "   Примеры включают обнаружение лиц или частей тела (глаза, нос и т. д.) на изображениях.*\n",
    "\n",
    "\n",
    "9. *Методы на основе генеративных моделей **(Generative Model-Based Methods)** :\\\n",
    "    Эти методы используют генеративные модели, такие как генеративные состязательные сети (GAN) или вариационные автокодировщики (VAE), для обнаружения объектов.\\\n",
    "    Они позволяют генерировать новые изображения, соответствующие заданным объектам.*\n",
    "\n",
    "\n",
    "10. *Методы на основе трекинга **(Tracking-Based Methods)** :\\\n",
    "    Эти методы комбинируют задачу обнаружения объектов с задачей трекинга, чтобы отслеживать объекты на видео.\\\n",
    "    Они используют информацию о движении объектов между кадрами для более надежного обнаружения и отслеживания.*\n",
    "\n",
    "\n",
    "\n",
    "Каждый из этих подходов имеет свои преимущества и ограничения и может быть эффективным в различных сценариях.\\\n",
    "Выбор определенного алгоритма зависит от требований к точности, скорости выполнения,\\\n",
    "доступных ресурсов и характеристик задачи обнаружения объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a744f-e701-4a4e-bae9-2b32c0e8e510",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------\n",
    "ВТОРОЕ  МНЕНИЕ\n",
    "\n",
    "Обнаружение объектов — это задача компьютерного зрения, которая включает обнаружение и локализацию объектов на изображении или видео.\\\n",
    "Существует несколько алгоритмов и методов, используемых для обнаружения объектов, и их можно разделить на следующие категории:\n",
    "\n",
    "\n",
    "1. *Подходы на основе скользящего окна: эти методы включают в себя скольжение окна фиксированного размера по изображению и классификацию каждого окна как содержащего объект или нет. Этот подход является дорогостоящим в вычислительном отношении и был в значительной степени заменен более эффективными алгоритмами.*\n",
    "\n",
    "2. *Подходы, основанные на предложениях регионов: эти алгоритмы сначала генерируют набор регионов-кандидатов, которые могут содержать объекты (предложения регионов), а затем классифицируют каждое предложение как содержащее объект или нет. Примеры включают выборочный поиск и EdgeBox.*\n",
    "\n",
    "3. *Детекторы одиночного выстрела (SSD): SSD — это тип метода обнаружения объектов на основе глубокого обучения, который выполняет как локализацию, так и классификацию за один проход через нейронную сеть. Он использует несколько сверточных слоев для прогнозирования ограничивающих рамок и вероятностей классов.*\n",
    "\n",
    "4. *Faster R-CNN: Faster R-CNN — это популярный метод обнаружения объектов, который представил сеть предложений регионов (RPN) для генерации предложений регионов вместо использования внешних алгоритмов. Он объединяет предложение региона и обнаружение объектов в единую сеть.*\n",
    "\n",
    "5. *YOLO (You Only Look Once): YOLO — это еще один алгоритм обнаружения объектов, основанный на глубоком обучении, который обрабатывает все изображение за один прямой проход через нейронную сеть и напрямую предсказывает ограничивающие рамки и вероятности классов.*\n",
    "\n",
    "6. *Mask R-CNN: Mask R-CNN расширяет Faster R-CNN, добавляя ветвь предсказания маски для создания масок сегментации объектов на уровне пикселей в дополнение к ограничивающим рамкам и вероятностям классов.*\n",
    "\n",
    "7. *EfficientDet: EfficientDet — это новейший современный алгоритм обнаружения объектов, который сочетает в себе эффективную сетевую архитектуру (EfficientNet) с двунаправленной пирамидальной сетью функций для достижения высокой точности и эффективности.*\n",
    "\n",
    "8. *CenterNet: CenterNet — это одноэтапный метод обнаружения объектов, который напрямую прогнозирует центры объектов и регрессирует ограничивающую рамку от центральной точки. Это эффективно и точно.*\n",
    "\n",
    "\n",
    "Это некоторые из популярных алгоритмов, используемых для обнаружения объектов. Каждый подход имеет свои сильные и слабые стороны, и выбор алгоритма зависит от таких факторов, как вычислительные ресурсы, скорость, требования к точности и характер приложения. Методы, основанные на глубоком обучении, особенно те, которые используют архитектуры CNN, в последние годы достигли значительных успехов и широко используются в современных системах обнаружения объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0ae48-4b5a-4e78-871f-b75fee52d911",
   "metadata": {},
   "source": [
    "Словарь:\n",
    "\n",
    "\n",
    "VGG — сверточная нейросетевая модель, предложенная К. Симоньяном и А. Зиссерманом из Оксфордского университета в работе «Very Deep Convolutional Networks for Large-Scale Image Recognition».  Модель достигает 92,7% точности топ-5 тестов в ImageNet  , который представляет собой набор данных из более чем 14 миллионов изображений, принадлежащих к 1000 классам.\n",
    "\n",
    "\n",
    "Dataset layout: CIFAR-10 (Компоновка набора данных)\\\n",
    "Python versions\\\n",
    "В архиве находятся файлы data_batch_1, data_batch_2, ..., data_batch_5, а также test_batch.\\\n",
    "Каждый из этих файлов представляет собой объект Python, созданный с помощью cPickle.\\\n",
    "Вот процедура python3, которая откроет такой файл и вернет словарь:\n",
    "\n",
    "def unpickle(file):\\\n",
    "    import pickle\\\n",
    "    with open(file, 'rb') as fo:\\\n",
    "        dict = pickle.load(fo, encoding='bytes')\\\n",
    "    return dict\n",
    "\n",
    "\n",
    "Загруженный таким образом, каждый пакетный файл содержит словарь со следующими элементами:\n",
    "data -- массив uint8 10000x3072 numpy. Каждая строка массива хранит цветное изображение 32x32. Первые 1024 записи содержат значения красного канала, следующие 1024 — зеленые, а последние 1024 — синие. Изображение хранится в порядке возрастания строк, так что первые 32 элемента массива являются значениями красного канала первой строки изображения.\n",
    "labels -- список из 10000 номеров в диапазоне от 0 до 9. Число в индексе i указывает на метку i-го изображения в массиве данных.\n",
    "\n",
    "Набор данных содержит еще один файл с именем batches.meta. Он также содержит объект словаря Python. В нем есть следующие записи:\n",
    "label_names -- список из 10 элементов, который дает осмысленные имена числовым меткам в массиве меток, описанном выше. Например, label_names[0] == «самолет», label_names[1] == «автомобиль» и т. д.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a31293f-5f24-4ff0-87e0-fa5ffb519bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ssd-tensorflow'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sergeyveneckiy/ssd-tensorflow.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a532caf-1992-4e39-9aae-241a4cb2d3de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    print(\"This is a Python 3 program. Use Python 3 or higher.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "314278f8-be52-483f-a07c-1327cb30b46f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 0: Загрузим необходимые библиотеки.\n",
    "\n",
    "import multiprocessing as mp\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import queue as q\n",
    "import lxml.etree\n",
    "import argparse\n",
    "import tarfile\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from transforms import *\n",
    "from ssdvgg import SSDVGG\n",
    "from math import sqrt, log, exp\n",
    "from data_queue import DataQueue\n",
    "from training_data import TrainingData\n",
    "from collections import namedtuple, defaultdict\n",
    "from average_precision import APCalculator, APs2mAP\n",
    "from utils import rgb2bgr, abs2prop, prop2abs, Label, Point, Box, Sample, Size, Overlap, Score, load_data_source, str2bool, draw_box, normalize_box\n",
    "from ssdutils import get_preset_by_name, get_anchors_for_preset, anchors2array, box2array, compute_overlap, compute_location, decode_boxes, suppress_overlaps\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2534ac-9a28-466c-a568-7ad35f3640be",
   "metadata": {},
   "source": [
    "Usage\n",
    "\n",
    "Чтобы обучить модель на данных Pascal VOC, перейдите в каталог pascal-voc и загрузите набор данных:\n",
    "cd pascal-voc\n",
    "./download-data.sh\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd8f0c1-02b7-4a0f-91d8-18446c2af895",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCtrainval_06-Nov-2007.tar already exists, skipping download.\n",
      "VOCtest_06-Nov-2007.tar already exists, skipping download.\n",
      "VOCtrainval_11-May-2012.tar already exists, skipping download.\n",
      "Extracting VOCtrainval_06-Nov-2007.tar...\n",
      "Extracting VOCtrainval_11-May-2012.tar...\n",
      "Extracting VOCtest_06-Nov-2007.tar...\n",
      "Data download and extraction complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Загрузка необходимых библиотек с помощью скрипта download-data.sh.\n",
    "#!/usr/bin/env python\n",
    "# URL-адреса для загрузки файлов tar пайтон-проекта из скрипта download-data.sh\n",
    "urls = [\n",
    "    \"http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\",\n",
    "    \"http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\",\n",
    "    \"http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar\"\n",
    "]\n",
    "# Download the tar files\n",
    "for url in urls:\n",
    "    filename = os.path.basename(url)\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    else:\n",
    "        print(f\"{filename} already exists, skipping download.\")\n",
    "\n",
    "# Create directories for trainval and test data\n",
    "os.makedirs(\"trainval\", exist_ok=True)\n",
    "os.makedirs(\"test\", exist_ok=True)\n",
    "\n",
    "# Extract the downloaded tar files\n",
    "print(\"Extracting VOCtrainval_06-Nov-2007.tar...\")\n",
    "with tarfile.open(\"VOCtrainval_06-Nov-2007.tar\", \"r\") as tar:\n",
    "    tar.extractall(\"trainval\")\n",
    "\n",
    "print(\"Extracting VOCtrainval_11-May-2012.tar...\")\n",
    "with tarfile.open(\"VOCtrainval_11-May-2012.tar\", \"r\") as tar:\n",
    "    tar.extractall(\"trainval\")\n",
    "\n",
    "print(\"Extracting VOCtest_06-Nov-2007.tar...\")\n",
    "with tarfile.open(\"VOCtest_06-Nov-2007.tar\", \"r\") as tar:\n",
    "    tar.extractall(\"test\")\n",
    "\n",
    "print(\"Data download and extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aecbc25-6cb1-454c-b3af-dfa05db24462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Load utils.py\n",
    "#-------------------------------------------------------------------------------\n",
    "def initialize_uninitialized_variables(sess):\n",
    "    \"\"\"\n",
    "    Only initialize the weights that have not yet been initialized by other\n",
    "    means, such as importing a metagraph and a checkpoint. It's useful when\n",
    "    extending an existing model.\n",
    "    \"\"\"\n",
    "    uninit_vars    = []\n",
    "    uninit_tensors = []\n",
    "    for var in tf.global_variables():\n",
    "        uninit_vars.append(var)\n",
    "        uninit_tensors.append(tf.is_variable_initialized(var))\n",
    "    uninit_bools = sess.run(uninit_tensors)\n",
    "    uninit = zip(uninit_bools, uninit_vars)\n",
    "    uninit = [var for init, var in uninit if not init]\n",
    "    sess.run(tf.variables_initializer(uninit))\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def load_data_source(data_source):\n",
    "    \"\"\"\n",
    "    Load a data source given it's name\n",
    "    \"\"\"\n",
    "    source_module = __import__('source_'+ data_source)\n",
    "    get_source    = getattr(source_module, 'get_source')\n",
    "    return get_source()\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def rgb2bgr(tpl):\n",
    "    \"\"\"\n",
    "    Convert RGB color tuple to BGR\n",
    "    \"\"\"\n",
    "    return (tpl[2], tpl[1], tpl[0])\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "Label   = namedtuple('Label',   ['name', 'color'])\n",
    "Size    = namedtuple('Size',    ['w', 'h'])\n",
    "Point   = namedtuple('Point',   ['x', 'y'])\n",
    "Sample  = namedtuple('Sample',  ['filename', 'boxes', 'imgsize'])\n",
    "Box     = namedtuple('Box',     ['label', 'labelid', 'center', 'size'])\n",
    "Score   = namedtuple('Score',   ['idx', 'score'])\n",
    "Overlap = namedtuple('Overlap', ['best', 'good'])\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def str2bool(v):\n",
    "    \"\"\"\n",
    "    Convert a string to a boolean\n",
    "    \"\"\"\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def abs2prop(xmin, xmax, ymin, ymax, imgsize):\n",
    "    \"\"\"\n",
    "    Convert the absolute min-max box bound to proportional center-width bounds\n",
    "    \"\"\"\n",
    "    width   = float(xmax-xmin)\n",
    "    height  = float(ymax-ymin)\n",
    "    cx      = float(xmin)+width/2\n",
    "    cy      = float(ymin)+height/2\n",
    "    width  /= imgsize.w\n",
    "    height /= imgsize.h\n",
    "    cx     /= imgsize.w\n",
    "    cy     /= imgsize.h\n",
    "    return Point(cx, cy), Size(width, height)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def prop2abs(center, size, imgsize):\n",
    "    \"\"\"\n",
    "    Convert proportional center-width bounds to absolute min-max bounds\n",
    "    \"\"\"\n",
    "    width2  = size.w*imgsize.w/2\n",
    "    height2 = size.h*imgsize.h/2\n",
    "    cx      = center.x*imgsize.w\n",
    "    cy      = center.y*imgsize.h\n",
    "    return int(cx-width2), int(cx+width2), int(cy-height2), int(cy+height2)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def box_is_valid(box):\n",
    "    for x in [box.center.x, box.center.y, box.size.w, box.size.h]:\n",
    "        if math.isnan(x) or math.isinf(x):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def normalize_box(box):\n",
    "    if not box_is_valid(box):\n",
    "        return box\n",
    "\n",
    "    img_size = Size(1000, 1000)\n",
    "    xmin, xmax, ymin, ymax = prop2abs(box.center, box.size, img_size)\n",
    "    xmin = max(xmin, 0)\n",
    "    xmax = min(xmax, img_size.w-1)\n",
    "    ymin = max(ymin, 0)\n",
    "    ymax = min(ymax, img_size.h-1)\n",
    "\n",
    "    # this happens early in the training when box min and max are outside\n",
    "    # of the image\n",
    "    xmin = min(xmin, xmax)\n",
    "    ymin = min(ymin, ymax)\n",
    "\n",
    "    center, size = abs2prop(xmin, xmax, ymin, ymax, img_size)\n",
    "    return Box(box.label, box.labelid, center, size)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def draw_box(img, box, color):\n",
    "    img_size = Size(img.shape[1], img.shape[0])\n",
    "    xmin, xmax, ymin, ymax = prop2abs(box.center, box.size, img_size)\n",
    "    img_box = np.copy(img)\n",
    "    cv2.rectangle(img_box, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "    cv2.rectangle(img_box, (xmin-1, ymin), (xmax+1, ymin-20), color, cv2.FILLED)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(img_box, box.label, (xmin+5, ymin-5), font, 0.5,\n",
    "                (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    alpha = 0.8\n",
    "    cv2.addWeighted(img_box, alpha, img, 1.-alpha, 0, img)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class PrecisionSummary:\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __init__(self, session, writer, sample_name, labels, restore=False):\n",
    "        self.session = session\n",
    "        self.writer = writer\n",
    "        self.labels = labels\n",
    "\n",
    "        sess = session\n",
    "        ph_name = sample_name+'_mAP_ph'\n",
    "        sum_name = sample_name+'_mAP'\n",
    "\n",
    "        if restore:\n",
    "            self.mAP_placeholder = sess.graph.get_tensor_by_name(ph_name+':0')\n",
    "            self.mAP_summary_op = sess.graph.get_tensor_by_name(sum_name+':0')\n",
    "        else:\n",
    "            self.mAP_placeholder = tf.placeholder(tf.float32, name=ph_name)\n",
    "            self.mAP_summary_op = tf.summary.scalar(sum_name,\n",
    "                                                    self.mAP_placeholder)\n",
    "\n",
    "        self.placeholders = {}\n",
    "        self.summary_ops = {}\n",
    "\n",
    "        for label in labels:\n",
    "            sum_name = sample_name+'_AP_'+label\n",
    "            ph_name = sample_name+'_AP_ph_'+label\n",
    "            if restore:\n",
    "                placeholder = sess.graph.get_tensor_by_name(ph_name+':0')\n",
    "                summary_op = sess.graph.get_tensor_by_name(sum_name+':0')\n",
    "            else:\n",
    "                placeholder = tf.placeholder(tf.float32, name=ph_name)\n",
    "                summary_op = tf.summary.scalar(sum_name, placeholder)\n",
    "            self.placeholders[label] = placeholder\n",
    "            self.summary_ops[label] = summary_op\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def push(self, epoch, mAP, APs):\n",
    "        if not APs: return\n",
    "\n",
    "        feed = {self.mAP_placeholder: mAP}\n",
    "        tensors = [self.mAP_summary_op]\n",
    "        for label in self.labels:\n",
    "            feed[self.placeholders[label]] = APs[label]\n",
    "            tensors.append(self.summary_ops[label])\n",
    "\n",
    "        summaries = self.session.run(tensors, feed_dict=feed)\n",
    "\n",
    "        for summary in summaries:\n",
    "            self.writer.add_summary(summary, epoch)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class ImageSummary:\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __init__(self, session, writer, sample_name, colors, restore=False):\n",
    "        self.session = session\n",
    "        self.writer = writer\n",
    "        self.colors = colors\n",
    "\n",
    "        sess = session\n",
    "        sum_name = sample_name+'_img'\n",
    "        ph_name = sample_name+'_img_ph'\n",
    "        if restore:\n",
    "            self.img_placeholder = sess.graph.get_tensor_by_name(ph_name+':0')\n",
    "            self.img_summary_op = sess.graph.get_tensor_by_name(sum_name+':0')\n",
    "        else:\n",
    "            self.img_placeholder = tf.placeholder(tf.float32, name=ph_name,\n",
    "                                                  shape=[None, None, None, 3])\n",
    "            self.img_summary_op = tf.summary.image(sum_name,\n",
    "                                                   self.img_placeholder)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def push(self, epoch, samples):\n",
    "        imgs = np.zeros((3, 512, 512, 3))\n",
    "        for i, sample in enumerate(samples):\n",
    "            img = cv2.resize(sample[0], (512, 512))\n",
    "            for _, box in sample[1]:\n",
    "                draw_box(img, box, self.colors[box.label])\n",
    "            img[img>255] = 255\n",
    "            img[img<0] = 0\n",
    "            imgs[i] = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        feed = {self.img_placeholder: imgs}\n",
    "        summary = self.session.run(self.img_summary_op, feed_dict=feed)\n",
    "        self.writer.add_summary(summary, epoch)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class LossSummary:\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __init__(self, session, writer, sample_name, num_samples,\n",
    "                 restore=False):\n",
    "        self.session = session\n",
    "        self.writer = writer\n",
    "        self.num_samples = num_samples\n",
    "        self.loss_names = ['total', 'localization', 'confidence', 'l2']\n",
    "        self.loss_values = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        sess = session\n",
    "\n",
    "        summary_ops = []\n",
    "        for loss in self.loss_names:\n",
    "            sum_name = sample_name+'_'+loss+'_loss'\n",
    "            ph_name = sample_name+'_'+loss+'_loss_ph'\n",
    "\n",
    "            if restore:\n",
    "                placeholder = sess.graph.get_tensor_by_name(ph_name+':0')\n",
    "                summary_op = sess.graph.get_tensor_by_name(sum_name+':0')\n",
    "            else:\n",
    "                placeholder = tf.placeholder(tf.float32, name=ph_name)\n",
    "                summary_op = tf.summary.scalar(sum_name, placeholder)\n",
    "\n",
    "            self.loss_values[loss] = float(0)\n",
    "            self.placeholders[loss] = placeholder\n",
    "            summary_ops.append(summary_op)\n",
    "\n",
    "        self.summary_ops = tf.summary.merge(summary_ops)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def add(self, values, num_samples):\n",
    "        for loss in self.loss_names:\n",
    "            self.loss_values[loss] += values[loss]*num_samples\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def push(self, epoch):\n",
    "        feed = {}\n",
    "        for loss in self.loss_names:\n",
    "            feed[self.placeholders[loss]] = \\\n",
    "                self.loss_values[loss]/self.num_samples\n",
    "\n",
    "        summary = self.session.run(self.summary_ops, feed_dict=feed)\n",
    "        self.writer.add_summary(summary, epoch)\n",
    "\n",
    "        for loss in self.loss_names:\n",
    "            self.loss_values[loss] = float(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8996ee-3341-48e6-810e-004fdb3ddc51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Load transforms.py\n",
    "#-------------------------------------------------------------------------------\n",
    "class Transform:\n",
    "    def __init__(self, **kwargs):\n",
    "        for arg, val in kwargs.items():\n",
    "            setattr(self, arg, val)\n",
    "        self.initialized = False\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class ImageLoaderTransform(Transform):\n",
    "    \"\"\"\n",
    "    Load and image from the file specified in the Sample object\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        return cv2.imread(gt.filename), label, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def process_overlap(overlap, box, anchor, matches, num_classes, vec):\n",
    "    if overlap.idx in matches and matches[overlap.idx] >= overlap.score:\n",
    "        return\n",
    "\n",
    "    matches[overlap.idx] = overlap.score\n",
    "    vec[overlap.idx, 0:num_classes+1] = 0\n",
    "    vec[overlap.idx, box.labelid]     = 1\n",
    "    vec[overlap.idx, num_classes+1:]  = compute_location(box, anchor)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class LabelCreatorTransform(Transform):\n",
    "    \"\"\"\n",
    "    Create a label vector out of a ground trut sample\n",
    "    Parameters: preset, num_classes\n",
    "    \"\"\"\n",
    "    #---------------------------------------------------------------------------\n",
    "    def initialize(self):\n",
    "        self.anchors = get_anchors_for_preset(self.preset)\n",
    "        self.vheight = len(self.anchors)\n",
    "        self.vwidth = self.num_classes+5 # background class + location offsets\n",
    "        self.img_size = Size(1000, 1000)\n",
    "        self.anchors_arr = anchors2array(self.anchors, self.img_size)\n",
    "        self.initialized = True\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __call__(self, data, label, gt):\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Initialize the data vector and other variables\n",
    "        #-----------------------------------------------------------------------\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "\n",
    "        vec = np.zeros((self.vheight, self.vwidth), dtype=np.float32)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # For every box compute the best match and all the matches above 0.5\n",
    "        # Jaccard overlap\n",
    "        #-----------------------------------------------------------------------\n",
    "        overlaps = {}\n",
    "        for box in gt.boxes:\n",
    "            box_arr = box2array(box, self.img_size)\n",
    "            overlaps[box] = compute_overlap(box_arr, self.anchors_arr, 0.5)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Set up the training vector resolving conflicts in favor of a better\n",
    "        # match\n",
    "        #-----------------------------------------------------------------------\n",
    "        vec[:, self.num_classes]   = 1 # background class\n",
    "        vec[:, self.num_classes+1] = 0 # x offset\n",
    "        vec[:, self.num_classes+2] = 0 # y offset\n",
    "        vec[:, self.num_classes+3] = 0 # log width scale\n",
    "        vec[:, self.num_classes+4] = 0 # log height scale\n",
    "\n",
    "        matches = {}\n",
    "        for box in gt.boxes:\n",
    "            for overlap in overlaps[box].good:\n",
    "                anchor = self.anchors[overlap.idx]\n",
    "                process_overlap(overlap, box, anchor, matches, self.num_classes, vec)\n",
    "\n",
    "        matches = {}\n",
    "        for box in gt.boxes:\n",
    "            overlap = overlaps[box].best\n",
    "            if not overlap:\n",
    "                continue\n",
    "            anchor  = self.anchors[overlap.idx]\n",
    "            process_overlap(overlap, box, anchor, matches, self.num_classes, vec)\n",
    "\n",
    "        return data, vec, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class ResizeTransform(Transform):\n",
    "    \"\"\"\n",
    "    Resize an image\n",
    "    Parameters: width, height, algorithms\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        alg = random.choice(self.algorithms)\n",
    "        resized = cv2.resize(data, (self.width, self.height), interpolation=alg)\n",
    "        return resized, label, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class RandomTransform(Transform):\n",
    "    \"\"\"\n",
    "    Call another transform with a given probability\n",
    "    Parameters: prob, transform\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        p = random.uniform(0, 1)\n",
    "        if p < self.prob:\n",
    "            return self.transform(data, label, gt)\n",
    "        return data, label, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class ComposeTransform(Transform):\n",
    "    \"\"\"\n",
    "    Call a bunch of transforms serially\n",
    "    Parameters: transforms\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        args = (data, label, gt)\n",
    "        for t in self.transforms:\n",
    "            args = t(*args)\n",
    "        return args\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class TransformPickerTransform(Transform):\n",
    "    \"\"\"\n",
    "    Call a randomly chosen transform from the list\n",
    "    Parameters: transforms\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        pick = random.randint(0, len(self.transforms)-1)\n",
    "        return self.transforms[pick](data, label, gt)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class BrightnessTransform(Transform):\n",
    "    \"\"\"\n",
    "    Transform brightness\n",
    "    Parameters: delta\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        data = data.astype(np.float32)\n",
    "        delta = random.randint(-self.delta, self.delta)\n",
    "        data += delta\n",
    "        data[data>255] = 255\n",
    "        data[data<0] = 0\n",
    "        data = data.astype(np.uint8)\n",
    "        return data, label, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class ContrastTransform(Transform):\n",
    "    \"\"\"\n",
    "    Transform contrast\n",
    "    Parameters: lower, upper\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        data = data.astype(np.float32)\n",
    "        delta = random.uniform(self.lower, self.upper)\n",
    "        data *= delta\n",
    "        data[data>255] = 255\n",
    "        data[data<0] = 0\n",
    "        data = data.astype(np.uint8)\n",
    "        return data, label, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class HueTransform(Transform):\n",
    "    \"\"\"\n",
    "    Transform hue\n",
    "    Parameters: delta\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        data = cv2.cvtColor(data, cv2.COLOR_BGR2HSV)\n",
    "        data = data.astype(np.float32)\n",
    "        delta = random.randint(-self.delta, self.delta)\n",
    "        data[0] += delta\n",
    "        data[0][data[0]>180] -= 180\n",
    "        data[0][data[0]<0] +=180\n",
    "        data = data.astype(np.uint8)\n",
    "        data = cv2.cvtColor(data, cv2.COLOR_HSV2BGR)\n",
    "        return data, label, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class SaturationTransform(Transform):\n",
    "    \"\"\"\n",
    "    Transform hue\n",
    "    Parameters: lower, upper\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        data = cv2.cvtColor(data, cv2.COLOR_BGR2HSV)\n",
    "        data = data.astype(np.float32)\n",
    "        delta = random.uniform(self.lower, self.upper)\n",
    "        data[1] *= delta\n",
    "        data[1][data[1]>255] = 255\n",
    "        data[1][data[1]<0] = 0\n",
    "        data = data.astype(np.uint8)\n",
    "        data = cv2.cvtColor(data, cv2.COLOR_HSV2BGR)\n",
    "        return data, label, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class ReorderChannelsTransform(Transform):\n",
    "    \"\"\"\n",
    "    Reorder Image Channels\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        channels = [0, 1, 2]\n",
    "        random.shuffle(channels)\n",
    "        return data[:, :,channels], label, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def transform_box(box, orig_size, new_size, h_off, w_off):\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Compute the new coordinates of the box\n",
    "    #---------------------------------------------------------------------------\n",
    "    xmin, xmax, ymin, ymax = prop2abs(box.center, box.size, orig_size)\n",
    "    xmin += w_off\n",
    "    xmax += w_off\n",
    "    ymin += h_off\n",
    "    ymax += h_off\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Check if the center falls within the image\n",
    "    #---------------------------------------------------------------------------\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    new_cx = xmin + int(width/2)\n",
    "    new_cy = ymin + int(height/2)\n",
    "    if new_cx < 0 or new_cx >= new_size.w:\n",
    "        return None\n",
    "    if new_cy < 0 or new_cy >= new_size.h:\n",
    "        return None\n",
    "\n",
    "    center, size = abs2prop(xmin, xmax, ymin, ymax, new_size)\n",
    "    return Box(box.label, box.labelid, center, size)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def transform_gt(gt, new_size, h_off, w_off):\n",
    "    boxes = []\n",
    "    for box in gt.boxes:\n",
    "        box = transform_box(box, gt.imgsize, new_size, h_off, w_off)\n",
    "        if box is None:\n",
    "            continue\n",
    "        boxes.append(box)\n",
    "    return Sample(gt.filename, boxes, new_size)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class ExpandTransform(Transform):\n",
    "    \"\"\"\n",
    "    Expand the image and fill the empty space with the mean value\n",
    "    Parameters: max_ratio, mean_value\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Calculate sizes and offsets\n",
    "        #-----------------------------------------------------------------------\n",
    "        ratio = random.uniform(1, self.max_ratio)\n",
    "        orig_size = gt.imgsize\n",
    "        new_size = Size(int(orig_size.w*ratio), int(orig_size.h*ratio))\n",
    "        h_off = random.randint(0, new_size.h-orig_size.h)\n",
    "        w_off = random.randint(0, new_size.w-orig_size.w)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Create the new image and place the input image in it\n",
    "        #-----------------------------------------------------------------------\n",
    "        img = np.zeros((new_size.h, new_size.w, 3))\n",
    "        img[:, :] = np.array(self.mean_value)\n",
    "        img[h_off:h_off+orig_size.h, w_off:w_off+orig_size.w, :] = data\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Transform the ground truth\n",
    "        #-----------------------------------------------------------------------\n",
    "        gt = transform_gt(gt, new_size, h_off, w_off)\n",
    "\n",
    "        return img, label, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class SamplerTransform(Transform):\n",
    "    \"\"\"\n",
    "    Sample a fraction of the image according to given parameters\n",
    "    Params: min_scale, max_scale, min_aspect_ratio, max_aspect_ratio,\n",
    "            min_jaccard_overlap\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Проверить, делать пробу или нет\n",
    "        #-----------------------------------------------------------------------\n",
    "        if not self.sample:\n",
    "            return data, label, gt\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Повторите выборку несколько раз\n",
    "        #-----------------------------------------------------------------------\n",
    "        source_boxes = anchors2array(gt.boxes, gt.imgsize)\n",
    "        box = None\n",
    "        box_arr = None\n",
    "        for _ in range(self.max_trials):\n",
    "            #-------------------------------------------------------------------\n",
    "            # Образец ограничивающей рамки\n",
    "            #-------------------------------------------------------------------\n",
    "            scale = random.uniform(self.min_scale, self.max_scale)\n",
    "            aspect_ratio = random.uniform(self.min_aspect_ratio,\n",
    "                                          self.max_aspect_ratio)\n",
    "\n",
    "            # make sure width and height will not be larger than 1\n",
    "            aspect_ratio = max(aspect_ratio, scale**2)\n",
    "            aspect_ratio = min(aspect_ratio, 1/(scale**2))\n",
    "\n",
    "            width = scale*sqrt(aspect_ratio)\n",
    "            height = scale/sqrt(aspect_ratio)\n",
    "            cx = 0.5*width + random.uniform(0, 1-width)\n",
    "            cy = 0.5*height + random.uniform(0, 1-height)\n",
    "            center = Point(cx, cy)\n",
    "            size = Size(width, height)\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Проверьте, удовлетворяет ли коробка ограничению перекрытия жаккарда.\n",
    "            #-------------------------------------------------------------------\n",
    "            box_arr = np.array(prop2abs(center, size, gt.imgsize))\n",
    "            overlap = compute_overlap(box_arr, source_boxes, 0)\n",
    "            if overlap.best and overlap.best.score >= self.min_jaccard_overlap:\n",
    "                box = Box(None, None, center, size)\n",
    "                break\n",
    "\n",
    "        if box is None:\n",
    "            return None\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Обрежьте поле и скорректируйте наземную правду\n",
    "        #-----------------------------------------------------------------------\n",
    "        new_size = Size(box_arr[1]-box_arr[0], box_arr[3]-box_arr[2])\n",
    "        w_off = -box_arr[0]\n",
    "        h_off = -box_arr[2]\n",
    "        data = data[box_arr[2]:box_arr[3], box_arr[0]:box_arr[1]]\n",
    "        gt = transform_gt(gt, new_size, h_off, w_off)\n",
    "\n",
    "        return data, label, gt\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class SamplePickerTransform(Transform):\n",
    "    \"\"\"\n",
    "    Run a bunch of sample transforms and return one of the produced samples\n",
    "    Parameters: samplers\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        samples = []\n",
    "        for sampler in self.samplers:\n",
    "            sample = sampler(data, label, gt)\n",
    "            if sample is not None:\n",
    "                samples.append(sample)\n",
    "        return random.choice(samples)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class HorizontalFlipTransform(Transform):\n",
    "    \"\"\"\n",
    "    Horizontally flip the image\n",
    "    \"\"\"\n",
    "    def __call__(self, data, label, gt):\n",
    "        data = cv2.flip(data, 1)\n",
    "        boxes = []\n",
    "        for box in gt.boxes:\n",
    "            center = Point(1-box.center.x, box.center.y)\n",
    "            box = Box(box.label, box.labelid, center, box.size)\n",
    "            boxes.append(box)\n",
    "        gt = Sample(gt.filename, boxes, gt.imgsize)\n",
    "\n",
    "        return data, label, gt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90cc2a93-74d8-484d-992e-7fc00b3c1622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Load ssdutils.py\n",
    "#-------------------------------------------------------------------------------\n",
    "# Определите разновидности SSD, которые мы собираемся использовать, и их различные свойства.\n",
    "# Это сделано для того, чтобы нам не приходилось строить всю сеть в памяти по порядку\n",
    "# для предварительной обработки наборов данных.\n",
    "#-------------------------------------------------------------------------------\n",
    "SSDMap = namedtuple('SSDMap', ['size', 'scale', 'aspect_ratios'])\n",
    "SSDPreset = namedtuple('SSDPreset', ['name', 'image_size', 'maps',\n",
    "                                     'extra_scale', 'num_anchors'])\n",
    "\n",
    "SSD_PRESETS = {\n",
    "    'vgg300': SSDPreset(name = 'vgg300',\n",
    "                        image_size = Size(300, 300),\n",
    "                        maps = [\n",
    "                            SSDMap(Size(38, 38), 0.1,   [2, 0.5]),\n",
    "                            SSDMap(Size(19, 19), 0.2,   [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size(10, 10), 0.375, [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size( 5,  5), 0.55,  [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size( 3,  3), 0.725, [2, 0.5]),\n",
    "                            SSDMap(Size( 1,  1), 0.9,   [2, 0.5])\n",
    "                        ],\n",
    "                        extra_scale = 1.075,\n",
    "                        num_anchors = 8732),\n",
    "    'vgg512': SSDPreset(name = 'vgg512',\n",
    "                        image_size = Size(512, 512),\n",
    "                        maps = [\n",
    "                            SSDMap(Size(64, 64), 0.07, [2, 0.5]),\n",
    "                            SSDMap(Size(32, 32), 0.15, [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size(16, 16), 0.3,  [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size( 8,  8), 0.45, [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size( 4,  4), 0.6,  [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size( 2,  2), 0.75, [2, 0.5]),\n",
    "                            SSDMap(Size( 1,  1), 0.9,  [2, 0.5])\n",
    "                        ],\n",
    "                        extra_scale = 1.05,\n",
    "                        num_anchors = 24564)\n",
    "}\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Параметры коробки по умолчанию как пропорциональны размерам изображения\n",
    "#-------------------------------------------------------------------------------\n",
    "Anchor = namedtuple('Anchor', ['center', 'size', 'x', 'y', 'scale', 'map'])\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def get_preset_by_name(pname):\n",
    "    if not pname in SSD_PRESETS:\n",
    "        raise RuntimeError('No such preset: '+pname)\n",
    "    return SSD_PRESETS[pname]\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def get_anchors_for_preset(preset):\n",
    "    \"\"\"\n",
    "    Compute the default (anchor) boxes for the given SSD preset\n",
    "    \"\"\"\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Вычислите ширину и высоту якорных ящиков для каждого масштаба.\n",
    "    #---------------------------------------------------------------------------\n",
    "    box_sizes = []\n",
    "    for i in range(len(preset.maps)):\n",
    "        map_params = preset.maps[i]\n",
    "        s = map_params.scale\n",
    "        aspect_ratios = [1] + map_params.aspect_ratios\n",
    "        aspect_ratios = list(map(lambda x: sqrt(x), aspect_ratios))\n",
    "\n",
    "        sizes = []\n",
    "        for ratio in aspect_ratios:\n",
    "            w = s * ratio\n",
    "            h = s / ratio\n",
    "            sizes.append((w, h))\n",
    "        if i < len(preset.maps)-1:\n",
    "            s_prime = sqrt(s*preset.maps[i+1].scale)\n",
    "        else:\n",
    "            s_prime = sqrt(s*preset.extra_scale)\n",
    "        sizes.append((s_prime, s_prime))\n",
    "        box_sizes.append(sizes)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Вычислить фактические поля для каждого масштаба и карты объектов\n",
    "    #---------------------------------------------------------------------------\n",
    "    anchors = []\n",
    "    for k in range(len(preset.maps)):\n",
    "        fk = preset.maps[k].size[0]\n",
    "        s = preset.maps[k].scale\n",
    "        for size in box_sizes[k]:\n",
    "            for j in range(fk):\n",
    "                y = (j+0.5)/float(fk)\n",
    "                for i in range(fk):\n",
    "                    x = (i+0.5)/float(fk)\n",
    "                    box = Anchor(Point(x, y), Size(size[0], size[1]),\n",
    "                                 i, j, s, k)\n",
    "                    anchors.append(box)\n",
    "    return anchors\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def anchors2array(anchors, img_size):\n",
    "    \"\"\"\n",
    "    Computes a numpy array out of absolute anchor params (img_size is needed\n",
    "    as a reference)\n",
    "    \"\"\"\n",
    "    arr = np.zeros((len(anchors), 4))\n",
    "    for i in range(len(anchors)):\n",
    "        anchor = anchors[i]\n",
    "        xmin, xmax, ymin, ymax = prop2abs(anchor.center, anchor.size, img_size)\n",
    "        arr[i] = np.array([xmin, xmax, ymin, ymax])\n",
    "    return arr\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def box2array(box, img_size):\n",
    "    xmin, xmax, ymin, ymax = prop2abs(box.center, box.size, img_size)\n",
    "    return np.array([xmin, xmax, ymin, ymax])\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def jaccard_overlap(box_arr, anchors_arr):\n",
    "    areaa = (anchors_arr[:, 1]-anchors_arr[:, 0]+1) * \\\n",
    "            (anchors_arr[:, 3]-anchors_arr[:, 2]+1)\n",
    "    areab = (box_arr[1]-box_arr[0]+1) * (box_arr[3]-box_arr[2]+1)\n",
    "\n",
    "    xxmin = np.maximum(box_arr[0], anchors_arr[:, 0])\n",
    "    xxmax = np.minimum(box_arr[1], anchors_arr[:, 1])\n",
    "    yymin = np.maximum(box_arr[2], anchors_arr[:, 2])\n",
    "    yymax = np.minimum(box_arr[3], anchors_arr[:, 3])\n",
    "\n",
    "    w = np.maximum(0, xxmax-xxmin+1)\n",
    "    h = np.maximum(0, yymax-yymin+1)\n",
    "    intersection = w*h\n",
    "    union = areab+areaa-intersection\n",
    "    return intersection/union\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def compute_overlap(box_arr, anchors_arr, threshold):\n",
    "    iou = jaccard_overlap(box_arr, anchors_arr)\n",
    "    overlap = iou > threshold\n",
    "\n",
    "    good_idxs = np.nonzero(overlap)[0]\n",
    "    best_idx  = np.argmax(iou)\n",
    "    best = None\n",
    "    good = []\n",
    "\n",
    "    if iou[best_idx] > threshold:\n",
    "        best = Score(best_idx, iou[best_idx])\n",
    "\n",
    "    for idx in good_idxs:\n",
    "        good.append(Score(idx, iou[idx]))\n",
    "\n",
    "    return Overlap(best, good)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def compute_location(box, anchor):\n",
    "    arr = np.zeros((4))\n",
    "    arr[0] = (box.center.x-anchor.center.x)/anchor.size.w*10\n",
    "    arr[1] = (box.center.y-anchor.center.y)/anchor.size.h*10\n",
    "    arr[2] = log(box.size.w/anchor.size.w)*5\n",
    "    arr[3] = log(box.size.h/anchor.size.h)*5\n",
    "    return arr\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def decode_location(box, anchor):\n",
    "    box[box > 100] = 100 # бывает только раннее обучение\n",
    "\n",
    "    x = box[0]/10 * anchor.size.w + anchor.center.x\n",
    "    y = box[1]/10 * anchor.size.h + anchor.center.y\n",
    "    w = exp(box[2]/5) * anchor.size.w\n",
    "    h = exp(box[3]/5) * anchor.size.h\n",
    "    return Point(x, y), Size(w, h)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def decode_boxes(pred, anchors, confidence_threshold = 0.01, lid2name = {},\n",
    "                 detections_cap=200):\n",
    "    \"\"\"\n",
    "    Расшифруйте блоки из предсказаний нейронной сети.\n",
    "    Имена меток расшифровываются с помощью словаря lid2name — id для имени\n",
    "    перевод не выполняется, если соответствующий ключ не существует.\n",
    "    \"\"\"\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Найдите обнаружения\n",
    "    #---------------------------------------------------------------------------\n",
    "    num_classes = pred.shape[1]-4\n",
    "    bg_class    = num_classes-1\n",
    "    box_class   = np.argmax(pred[:, :num_classes-1], axis=1)\n",
    "    confidence  = pred[np.arange(len(pred)), box_class]\n",
    "    if detections_cap is not None:\n",
    "        detections = np.argsort(confidence)[::-1][:detections_cap]\n",
    "    else:\n",
    "        detections = np.argsort(confidence)[::-1]\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Декодируйте координаты каждого ящика с уверенностью выше порога\n",
    "    #---------------------------------------------------------------------------\n",
    "    boxes = []\n",
    "    for idx in detections:\n",
    "        confidence = pred[idx, box_class[idx]]\n",
    "        if confidence < confidence_threshold:\n",
    "            break\n",
    "\n",
    "        center, size = decode_location(pred[idx, num_classes:], anchors[idx])\n",
    "        cid = box_class[idx]\n",
    "        cname = None\n",
    "        if cid in lid2name:\n",
    "            cname = lid2name[cid]\n",
    "        det = (confidence, normalize_box(Box(cname, cid, center, size)))\n",
    "        boxes.append(det)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def non_maximum_suppression(boxes, overlap_threshold):\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Преобразование в абсолютные координаты и в более удобный формат\n",
    "    #---------------------------------------------------------------------------\n",
    "    xmin = []\n",
    "    xmax = []\n",
    "    ymin = []\n",
    "    ymax = []\n",
    "    conf = []\n",
    "    img_size = Size(1000, 1000)\n",
    "\n",
    "    for box in boxes:\n",
    "        params = prop2abs(box[1].center, box[1].size, img_size)\n",
    "        xmin.append(params[0])\n",
    "        xmax.append(params[1])\n",
    "        ymin.append(params[2])\n",
    "        ymax.append(params[3])\n",
    "        conf.append(box[0])\n",
    "\n",
    "    xmin = np.array(xmin)\n",
    "    xmax = np.array(xmax)\n",
    "    ymin = np.array(ymin)\n",
    "    ymax = np.array(ymax)\n",
    "    conf = np.array(conf)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Вычислите площадь каждого ящика и отсортируйте индексы по уровню достоверности.\n",
    "    # (сначала самая низкая достоверность).\n",
    "    #---------------------------------------------------------------------------\n",
    "    area = (xmax-xmin+1) * (ymax-ymin+1)\n",
    "    idxs = np.argsort(conf)\n",
    "    pick = []\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Цикл, пока у нас не останутся индексы для обработки\n",
    "    #---------------------------------------------------------------------------\n",
    "    while len(idxs) > 0:\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Возьмите последний индекс (т.е. наиболее надежное обнаружение), удалите его из\n",
    "        # список индексов для обработки и помещаем в список пикировок\n",
    "        #-----------------------------------------------------------------------\n",
    "        last = idxs.shape[0]-1\n",
    "        i    = idxs[last]\n",
    "        idxs = np.delete(idxs, last)\n",
    "        pick.append(i)\n",
    "        suppress = []\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Выясните пересечение с оставшимися окнами\n",
    "        #-----------------------------------------------------------------------\n",
    "        xxmin = np.maximum(xmin[i], xmin[idxs])\n",
    "        xxmax = np.minimum(xmax[i], xmax[idxs])\n",
    "        yymin = np.maximum(ymin[i], ymin[idxs])\n",
    "        yymax = np.minimum(ymax[i], ymax[idxs])\n",
    "\n",
    "        w = np.maximum(0, xxmax-xxmin+1)\n",
    "        h = np.maximum(0, yymax-yymin+1)\n",
    "        intersection = w*h\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Вычислять IOU и подавлять индексы с IOU выше порогового значени\n",
    "        #-----------------------------------------------------------------------\n",
    "        union    = area[i]+area[idxs]-intersection\n",
    "        iou      = intersection/union\n",
    "        overlap  = iou > overlap_threshold\n",
    "        suppress = np.nonzero(overlap)[0]\n",
    "        idxs     = np.delete(idxs, suppress)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Вернуть выбранные ящики\n",
    "    #---------------------------------------------------------------------------\n",
    "    selected = []\n",
    "    for i in pick:\n",
    "        selected.append(boxes[i])\n",
    "\n",
    "    return selected\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def suppress_overlaps(boxes):\n",
    "    class_boxes    = defaultdict(list)\n",
    "    selected_boxes = []\n",
    "    for box in boxes:\n",
    "        class_boxes[box[1].labelid].append(box)\n",
    "\n",
    "    for k, v in class_boxes.items():\n",
    "        selected_boxes += non_maximum_suppression(v, 0.45)\n",
    "    return selected_boxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "899375bd-c454-4099-aa1d-b6d40f490688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Load source_voc.py (if available) - Зависит от используемого источника данных\n",
    "#-------------------------------------------------------------------------------\n",
    "# Labels\n",
    "#-------------------------------------------------------------------------------\n",
    "label_defs = [\n",
    "    Label('aeroplane',   rgb2bgr((0,     0,   0))),\n",
    "    Label('bicycle',     rgb2bgr((111,  74,   0))),\n",
    "    Label('bird',        rgb2bgr(( 81,   0,  81))),\n",
    "    Label('boat',        rgb2bgr((128,  64, 128))),\n",
    "    Label('bottle',      rgb2bgr((244,  35, 232))),\n",
    "    Label('bus',         rgb2bgr((230, 150, 140))),\n",
    "    Label('car',         rgb2bgr(( 70,  70,  70))),\n",
    "    Label('cat',         rgb2bgr((102, 102, 156))),\n",
    "    Label('chair',       rgb2bgr((190, 153, 153))),\n",
    "    Label('cow',         rgb2bgr((150, 120,  90))),\n",
    "    Label('diningtable', rgb2bgr((153, 153, 153))),\n",
    "    Label('dog',         rgb2bgr((250, 170,  30))),\n",
    "    Label('horse',       rgb2bgr((220, 220,   0))),\n",
    "    Label('motorbike',   rgb2bgr((107, 142,  35))),\n",
    "    Label('person',      rgb2bgr(( 52, 151,  52))),\n",
    "    Label('pottedplant', rgb2bgr(( 70, 130, 180))),\n",
    "    Label('sheep',       rgb2bgr((220,  20,  60))),\n",
    "    Label('sofa',        rgb2bgr((  0,   0, 142))),\n",
    "    Label('train',       rgb2bgr((  0,   0, 230))),\n",
    "    Label('tvmonitor',   rgb2bgr((119,  11,  32)))]\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class PascalVOCSource:\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __init__(self):\n",
    "        self.num_classes   = len(label_defs)\n",
    "        self.colors        = {l.name: l.color for l in label_defs}\n",
    "        self.lid2name      = {i: l.name for i, l in enumerate(label_defs)}\n",
    "        self.lname2id      = {l.name: i for i, l in enumerate(label_defs)}\n",
    "        self.num_train     = 0\n",
    "        self.num_valid     = 0\n",
    "        self.num_test      = 0\n",
    "        self.train_samples = []\n",
    "        self.valid_samples = []\n",
    "        self.test_samples  = []\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __build_annotation_list(self, root, dataset_type):\n",
    "        \"\"\"\n",
    "        Build a list of samples for the VOC dataset (either trainval or test)\n",
    "        \"\"\"\n",
    "        annot_root  = root + '/Annotations/'\n",
    "        annot_files = []\n",
    "        with open(root + '/ImageSets/Main/' + dataset_type + '.txt') as f:\n",
    "            for line in f:\n",
    "                annot_file = annot_root + line.strip() + '.xml'\n",
    "                if os.path.exists(annot_file):\n",
    "                    annot_files.append(annot_file)\n",
    "        return annot_files\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __build_sample_list(self, root, annot_files, dataset_name):\n",
    "        \"\"\"\n",
    "        Build a list of samples for the VOC dataset (either trainval or test)\n",
    "        \"\"\"\n",
    "        image_root  = root + '/JPEGImages/'\n",
    "        samples     = []\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Process each annotated sample\n",
    "        #-----------------------------------------------------------------------\n",
    "        for fn in tqdm(annot_files, desc=dataset_name, unit='samples'):\n",
    "            with open(fn, 'r') as f:\n",
    "                doc      = lxml.etree.parse(f)\n",
    "                filename = image_root+doc.xpath('/annotation/filename')[0].text\n",
    "\n",
    "                #---------------------------------------------------------------\n",
    "                # Get the file dimensions\n",
    "                #---------------------------------------------------------------\n",
    "                if not os.path.exists(filename):\n",
    "                    continue\n",
    "\n",
    "                img     = cv2.imread(filename)\n",
    "                imgsize = Size(img.shape[1], img.shape[0])\n",
    "\n",
    "                #---------------------------------------------------------------\n",
    "                # Get boxes for all the objects\n",
    "                #---------------------------------------------------------------\n",
    "                boxes    = []\n",
    "                objects  = doc.xpath('/annotation/object')\n",
    "                for obj in objects:\n",
    "                    #-----------------------------------------------------------\n",
    "                    # Get the properties of the box and convert them to the\n",
    "                    # proportional terms\n",
    "                    #-----------------------------------------------------------\n",
    "                    label = obj.xpath('name')[0].text\n",
    "                    xmin  = int(float(obj.xpath('bndbox/xmin')[0].text))\n",
    "                    xmax  = int(float(obj.xpath('bndbox/xmax')[0].text))\n",
    "                    ymin  = int(float(obj.xpath('bndbox/ymin')[0].text))\n",
    "                    ymax  = int(float(obj.xpath('bndbox/ymax')[0].text))\n",
    "                    center, size = abs2prop(xmin, xmax, ymin, ymax, imgsize)\n",
    "                    box = Box(label, self.lname2id[label], center, size)\n",
    "                    boxes.append(box)\n",
    "                if not boxes:\n",
    "                    continue\n",
    "                sample = Sample(filename, boxes, imgsize)\n",
    "                samples.append(sample)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def load_trainval_data(self, data_dir, valid_fraction):\n",
    "        \"\"\"\n",
    "        Load the training and validation data\n",
    "        :param data_dir:       the directory where the dataset's file are stored\n",
    "        :param valid_fraction: what franction of the dataset should be used\n",
    "                               as a validation sample\n",
    "        \"\"\"\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Process the samples defined in the relevant file lists\n",
    "        #-----------------------------------------------------------------------\n",
    "        train_annot = []\n",
    "        train_samples = []\n",
    "        for vocid in ['VOC2007', 'VOC2012']:\n",
    "            root = data_dir + '/trainval/VOCdevkit/'+vocid\n",
    "            name = 'trainval_'+vocid\n",
    "            annot = self.__build_annotation_list(root, 'trainval')\n",
    "            train_annot += annot\n",
    "            train_samples += self.__build_sample_list(root, annot, name)\n",
    "\n",
    "        root = data_dir + '/test/VOCdevkit/VOC2007'\n",
    "        annot = self.__build_annotation_list(root, 'test')\n",
    "        train_samples += self.__build_sample_list(root, annot, 'test_VOC2007')\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # We have some 5.5k annotated samples that are not on these lists, so\n",
    "        # we can use them for validation\n",
    "        #-----------------------------------------------------------------------\n",
    "        root = data_dir + '/trainval/VOCdevkit/VOC2012'\n",
    "        all_annot = set(glob(root + '/Annotations/*.xml'))\n",
    "        valid_annot = all_annot - set(train_annot)\n",
    "        valid_samples = self.__build_sample_list(root, valid_annot,\n",
    "                                                 'valid_VOC2012')\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Final set up and sanity check\n",
    "        #-----------------------------------------------------------------------\n",
    "        self.valid_samples = valid_samples\n",
    "        self.train_samples = train_samples\n",
    "\n",
    "        if len(self.train_samples) == 0:\n",
    "            raise RuntimeError('No training samples found in ' + data_dir)\n",
    "\n",
    "        if valid_fraction > 0:\n",
    "            if len(self.valid_samples) == 0:\n",
    "                raise RuntimeError('No validation samples found in ' + data_dir)\n",
    "\n",
    "        self.num_train = len(self.train_samples)\n",
    "        self.num_valid = len(self.valid_samples)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def load_test_data(self, data_dir):\n",
    "        \"\"\"\n",
    "        Load the test data\n",
    "        :param data_dir: the directory where the dataset's file are stored\n",
    "        \"\"\"\n",
    "        root = data_dir + '/test/VOCdevkit/VOC2012'\n",
    "        annot = self.__build_annotation_list(root, 'test')\n",
    "        self.test_samples  = self.__build_sample_list(root, annot,\n",
    "                                                      'test_VOC2012')\n",
    "\n",
    "        if len(self.test_samples) == 0:\n",
    "            raise RuntimeError('No testing samples found in ' + data_dir)\n",
    "\n",
    "        self.num_test  = len(self.test_samples)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def get_source():\n",
    "    return PascalVOCSource()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d660fcde-e885-4910-b227-ebacada284ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 6: Load data_queue.py\n",
    "#-------------------------------------------------------------------------------\n",
    "class DataQueue:\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __init__(self, img_template, label_template, maxsize):\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Выясните кортежи данных, размеры и формы обоих массивов.\n",
    "        #-----------------------------------------------------------------------\n",
    "        self.img_dtype = img_template.dtype\n",
    "        self.img_shape = img_template.shape\n",
    "        self.img_bc = len(img_template.tobytes())\n",
    "        self.label_dtype = label_template.dtype\n",
    "        self.label_shape = label_template.shape\n",
    "        self.label_bc = len(label_template.tobytes())\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Создайте пул массивов и очередь\n",
    "        #-----------------------------------------------------------------------\n",
    "        self.array_pool = []\n",
    "        self.array_queue = mp.Queue(maxsize)\n",
    "        for i in range(maxsize):\n",
    "            img_buff = mp.Array('c', self.img_bc, lock=False)\n",
    "            img_arr = np.frombuffer(img_buff, dtype=self.img_dtype)\n",
    "            img_arr = img_arr.reshape(self.img_shape)\n",
    "\n",
    "            label_buff = mp.Array('c', self.label_bc, lock=False)\n",
    "            label_arr = np.frombuffer(label_buff, dtype=self.label_dtype)\n",
    "            label_arr = label_arr.reshape(self.label_shape)\n",
    "\n",
    "            self.array_pool.append((img_arr, label_arr))\n",
    "            self.array_queue.put(i)\n",
    "\n",
    "        self.queue = mp.Queue(maxsize)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def put(self, img, label, boxes, *args, **kwargs):\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Проверьте, соответствуют ли параметры данным, которые мы можем хранить\n",
    "        #-----------------------------------------------------------------------\n",
    "        def check_consistency(name, arr, dtype, shape, byte_count):\n",
    "            if type(arr) is not np.ndarray:\n",
    "                raise ValueError(name + ' needs to be a numpy array')\n",
    "            if arr.dtype != dtype:\n",
    "                raise ValueError('{}\\'s elements need to be of type {} but is {}' \\\n",
    "                                 .format(name, str(dtype), str(arr.dtype)))\n",
    "            if arr.shape != shape:\n",
    "                raise ValueError('{}\\'s shape needs to be {} but is {}' \\\n",
    "                                 .format(name, shape, arr.shape))\n",
    "            if len(arr.tobytes()) != byte_count:\n",
    "                raise ValueError('{}\\'s byte count needs to be {} but is {}' \\\n",
    "                                 .format(name, byte_count, len(arr.data)))\n",
    "\n",
    "        check_consistency('img', img, self.img_dtype, self.img_shape,\n",
    "                          self.img_bc)\n",
    "        check_consistency('label', label, self.label_dtype, self.label_shape,\n",
    "                          self.label_bc)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Если мы не можем получить слот в течение тайм-аута, мы на самом деле заполнены, а не\n",
    "        # пустой\n",
    "        #-----------------------------------------------------------------------\n",
    "        try:\n",
    "            arr_id = self.array_queue.get(*args, **kwargs)\n",
    "        except q.Empty:\n",
    "            raise q.Full()\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Скопируйте массивы в общий пул\n",
    "        #-----------------------------------------------------------------------\n",
    "        self.array_pool[arr_id][0][:] = img\n",
    "        self.array_pool[arr_id][1][:] = label\n",
    "        self.queue.put((arr_id, boxes), *args, **kwargs)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def get(self, *args, **kwargs):\n",
    "        item = self.queue.get(*args, **kwargs)\n",
    "        arr_id = item[0]\n",
    "        boxes = item[1]\n",
    "\n",
    "        img = np.copy(self.array_pool[arr_id][0])\n",
    "        label = np.copy(self.array_pool[arr_id][1])\n",
    "\n",
    "        self.array_queue.put(arr_id)\n",
    "\n",
    "        return img, label, boxes\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def empty(self):\n",
    "        return self.queue.empty()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7793122-79f7-4038-9295-733e08b8bdea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 7: Load training_data.py\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "class TrainingData:\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __init__(self, data_dir):\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Прочитать информацию о наборе данных\n",
    "        #-----------------------------------------------------------------------\n",
    "        try:\n",
    "            with open(data_dir+'/training-data.pkl', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            with open(data_dir+'/train-samples.pkl', 'rb') as f:\n",
    "                train_samples = pickle.load(f)\n",
    "            with open(data_dir+'/valid-samples.pkl', 'rb') as f:\n",
    "                valid_samples = pickle.load(f)\n",
    "        except (FileNotFoundError, IOError) as e:\n",
    "            raise RuntimeError(str(e))\n",
    "\n",
    "        nones = [None] * len(train_samples)\n",
    "        train_samples = list(zip(nones, nones, train_samples))\n",
    "        nones = [None] * len(valid_samples)\n",
    "        valid_samples = list(zip(nones, nones, valid_samples))\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Настройте атрибуты\n",
    "        #-----------------------------------------------------------------------\n",
    "        self.preset          = data['preset']\n",
    "        self.num_classes     = data['num-classes']\n",
    "        self.label_colors    = data['colors']\n",
    "        self.lid2name        = data['lid2name']\n",
    "        self.lname2id        = data['lname2id']\n",
    "        self.train_tfs       = data['train-transforms']\n",
    "        self.valid_tfs       = data['valid-transforms']\n",
    "        self.train_generator = self.__batch_generator(train_samples,\n",
    "                                                      self.train_tfs)\n",
    "        self.valid_generator = self.__batch_generator(valid_samples,\n",
    "                                                      self.valid_tfs)\n",
    "        self.num_train       = len(train_samples)\n",
    "        self.num_valid       = len(valid_samples)\n",
    "        self.train_samples   = list(map(lambda x: x[2], train_samples))\n",
    "        self.valid_samples   = list(map(lambda x: x[2], valid_samples))\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    def __batch_generator(self, sample_list_, transforms):\n",
    "        image_size = (self.preset.image_size.w, self.preset.image_size.h)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        def run_transforms(sample):\n",
    "            args = sample\n",
    "            for t in transforms:\n",
    "                args = t(*args)\n",
    "            return args\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        def process_samples(samples):\n",
    "            images = []\n",
    "            labels = []\n",
    "            gt_boxes = []\n",
    "            for s in samples:\n",
    "                done = False\n",
    "                counter = 0\n",
    "                while not done and counter < 50:\n",
    "                    image, label, gt = run_transforms(s)\n",
    "                    num_bg = np.count_nonzero(label[:, self.num_classes])\n",
    "                    done = num_bg < label.shape[0]\n",
    "                    counter += 1\n",
    "\n",
    "                images.append(image.astype(np.float32))\n",
    "                labels.append(label.astype(np.float32))\n",
    "                gt_boxes.append(gt.boxes)\n",
    "\n",
    "            images = np.array(images, dtype=np.float32)\n",
    "            labels = np.array(labels, dtype=np.float32)\n",
    "            return images, labels, gt_boxes\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        def batch_producer(sample_queue, batch_queue):\n",
    "            while True:\n",
    "                #---------------------------------------------------------------\n",
    "                # Обработать образец\n",
    "                #---------------------------------------------------------------\n",
    "                try:\n",
    "                    samples = sample_queue.get(timeout=1)\n",
    "                except q.Empty:\n",
    "                    break\n",
    "\n",
    "                images, labels, gt_boxes = process_samples(samples)\n",
    "\n",
    "                #---------------------------------------------------------------\n",
    "                # Дополнить результат в случае, если у нас недостаточно сэмплов\n",
    "                # чтобы заполнить всю партию\n",
    "                #---------------------------------------------------------------\n",
    "                if images.shape[0] < batch_queue.img_shape[0]:\n",
    "                    images_norm = np.zeros(batch_queue.img_shape,\n",
    "                                           dtype=np.float32)\n",
    "                    labels_norm = np.zeros(batch_queue.label_shape,\n",
    "                                           dtype=np.float32)\n",
    "                    images_norm[:images.shape[0]] = images\n",
    "                    labels_norm[:images.shape[0]] = labels\n",
    "                    batch_queue.put(images_norm, labels_norm, gt_boxes)\n",
    "                else:\n",
    "                    batch_queue.put(images, labels, gt_boxes)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        def gen_batch(batch_size, num_workers=0):\n",
    "            sample_list = copy(sample_list_)\n",
    "            random.shuffle(sample_list)\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Настроить параллельный генератор\n",
    "            #-------------------------------------------------------------------\n",
    "            if num_workers > 0:\n",
    "                #---------------------------------------------------------------\n",
    "                # Настройка очередей\n",
    "                #---------------------------------------------------------------\n",
    "                img_template = np.zeros((batch_size, self.preset.image_size.h,\n",
    "                                         self.preset.image_size.w, 3),\n",
    "                                        dtype=np.float32)\n",
    "                label_template = np.zeros((batch_size, self.preset.num_anchors,\n",
    "                                           self.num_classes+5),\n",
    "                                          dtype=np.float32)\n",
    "                max_size = num_workers*5\n",
    "                n_batches = int(math.ceil(len(sample_list_)/batch_size))\n",
    "                sample_queue = mp.Queue(n_batches)\n",
    "                batch_queue = DataQueue(img_template, label_template, max_size)\n",
    "\n",
    "                #---------------------------------------------------------------\n",
    "                # Настроить рабочих. Убедитесь, что мы можем безопасно разветвляться, даже если\n",
    "                # OpenCV был скомпилирован с использованием CUDA и многопоточности.\n",
    "                # поддерживать.\n",
    "                #---------------------------------------------------------------\n",
    "                workers = []\n",
    "                os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "                cv2_num_threads = cv2.getNumThreads()\n",
    "                cv2.setNumThreads(1)\n",
    "                for i in range(num_workers):\n",
    "                    args = (sample_queue, batch_queue)\n",
    "                    w = mp.Process(target=batch_producer, args=args)\n",
    "                    workers.append(w)\n",
    "                    w.start()\n",
    "                del os.environ['CUDA_VISIBLE_DEVICES']\n",
    "                cv2.setNumThreads(cv2_num_threads)\n",
    "\n",
    "                #---------------------------------------------------------------\n",
    "                # Заполните очередь образцов данными\n",
    "                #---------------------------------------------------------------\n",
    "                for offset in range(0, len(sample_list), batch_size):\n",
    "                    samples = sample_list[offset:offset+batch_size]\n",
    "                    sample_queue.put(samples)\n",
    "\n",
    "                #---------------------------------------------------------------\n",
    "                # Вернуть данные\n",
    "                #---------------------------------------------------------------\n",
    "                for offset in range(0, len(sample_list), batch_size):\n",
    "                    images, labels, gt_boxes = batch_queue.get()\n",
    "                    num_items = len(gt_boxes)\n",
    "                    yield images[:num_items], labels[:num_items], gt_boxes\n",
    "\n",
    "                #---------------------------------------------------------------\n",
    "                # Присоединяйтесь к рабочим\n",
    "                #---------------------------------------------------------------\n",
    "                for w in workers:\n",
    "                    w.join()\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Возврат серийного генератора\n",
    "            #-------------------------------------------------------------------\n",
    "            else:\n",
    "                for offset in range(0, len(sample_list), batch_size):\n",
    "                    samples = sample_list[offset:offset+batch_size]\n",
    "                    images, labels, gt_boxes = process_samples(samples)\n",
    "                    yield images, labels, gt_boxes\n",
    "\n",
    "        return gen_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89fc1bc6-a8b1-4a8f-ab6c-5410efee64bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 8: Load generator.py (if available) - Зависит от используемого источника данных\n",
    "# [Code from generator.py]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6754a1a3-cabe-470b-82da-9040618e80f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 9: Load solver.py (if available) - Зависит от используемой сетевой архитектуры\n",
    "# [Code from solver.py]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d42ac9f-9573-4672-b385-f51295ed5873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 10: Load ssd.py (if available) - Зависит от используемой сетевой архитектуры\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Определите разновидности SSD, которые мы собираемся использовать, и их различные свойства.\n",
    "# Это сделано для того, чтобы нам не приходилось строить всю сеть в памяти по порядку\n",
    "# для предварительной обработки наборов данных.\n",
    "#-------------------------------------------------------------------------------\n",
    "SSDMap = namedtuple('SSDMap', ['size', 'scale', 'aspect_ratios'])\n",
    "SSDPreset = namedtuple('SSDPreset', ['name', 'image_size', 'maps',\n",
    "                                     'extra_scale', 'num_anchors'])\n",
    "\n",
    "SSD_PRESETS = {\n",
    "    'vgg300': SSDPreset(name = 'vgg300',\n",
    "                        image_size = Size(300, 300),\n",
    "                        maps = [\n",
    "                            SSDMap(Size(38, 38), 0.1,   [2, 0.5]),\n",
    "                            SSDMap(Size(19, 19), 0.2,   [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size(10, 10), 0.375, [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size( 5,  5), 0.55,  [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size( 3,  3), 0.725, [2, 0.5]),\n",
    "                            SSDMap(Size( 1,  1), 0.9,   [2, 0.5])\n",
    "                        ],\n",
    "                        extra_scale = 1.075,\n",
    "                        num_anchors = 8732),\n",
    "    'vgg512': SSDPreset(name = 'vgg512',\n",
    "                        image_size = Size(512, 512),\n",
    "                        maps = [\n",
    "                            SSDMap(Size(64, 64), 0.07, [2, 0.5]),\n",
    "                            SSDMap(Size(32, 32), 0.15, [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size(16, 16), 0.3,  [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size( 8,  8), 0.45, [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size( 4,  4), 0.6,  [2, 3, 0.5, 1./3.]),\n",
    "                            SSDMap(Size( 2,  2), 0.75, [2, 0.5]),\n",
    "                            SSDMap(Size( 1,  1), 0.9,  [2, 0.5])\n",
    "                        ],\n",
    "                        extra_scale = 1.05,\n",
    "                        num_anchors = 24564)\n",
    "}\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Параметры коробки по умолчанию как пропорциональны размерам изображения\n",
    "#-------------------------------------------------------------------------------\n",
    "Anchor = namedtuple('Anchor', ['center', 'size', 'x', 'y', 'scale', 'map'])\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def get_preset_by_name(pname):\n",
    "    if not pname in SSD_PRESETS:\n",
    "        raise RuntimeError('No such preset: '+pname)\n",
    "    return SSD_PRESETS[pname]\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def get_anchors_for_preset(preset):\n",
    "    \"\"\"\n",
    "    Compute the default (anchor) boxes for the given SSD preset\n",
    "    \"\"\"\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Вычислите ширину и высоту якорных ящиков для каждого масштаба.\n",
    "    #---------------------------------------------------------------------------\n",
    "    box_sizes = []\n",
    "    for i in range(len(preset.maps)):\n",
    "        map_params = preset.maps[i]\n",
    "        s = map_params.scale\n",
    "        aspect_ratios = [1] + map_params.aspect_ratios\n",
    "        aspect_ratios = list(map(lambda x: sqrt(x), aspect_ratios))\n",
    "\n",
    "        sizes = []\n",
    "        for ratio in aspect_ratios:\n",
    "            w = s * ratio\n",
    "            h = s / ratio\n",
    "            sizes.append((w, h))\n",
    "        if i < len(preset.maps)-1:\n",
    "            s_prime = sqrt(s*preset.maps[i+1].scale)\n",
    "        else:\n",
    "            s_prime = sqrt(s*preset.extra_scale)\n",
    "        sizes.append((s_prime, s_prime))\n",
    "        box_sizes.append(sizes)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Вычислить фактические поля для каждого масштаба и карты объектов\n",
    "    #---------------------------------------------------------------------------\n",
    "    anchors = []\n",
    "    for k in range(len(preset.maps)):\n",
    "        fk = preset.maps[k].size[0]\n",
    "        s = preset.maps[k].scale\n",
    "        for size in box_sizes[k]:\n",
    "            for j in range(fk):\n",
    "                y = (j+0.5)/float(fk)\n",
    "                for i in range(fk):\n",
    "                    x = (i+0.5)/float(fk)\n",
    "                    box = Anchor(Point(x, y), Size(size[0], size[1]),\n",
    "                                 i, j, s, k)\n",
    "                    anchors.append(box)\n",
    "    return anchors\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def anchors2array(anchors, img_size):\n",
    "    \"\"\"\n",
    "    Вычисляет массив numpy из абсолютных параметров привязки (необходим img_size\n",
    "    в качестве ссылки)\n",
    "    \"\"\"\n",
    "    arr = np.zeros((len(anchors), 4))\n",
    "    for i in range(len(anchors)):\n",
    "        anchor = anchors[i]\n",
    "        xmin, xmax, ymin, ymax = prop2abs(anchor.center, anchor.size, img_size)\n",
    "        arr[i] = np.array([xmin, xmax, ymin, ymax])\n",
    "    return arr\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def box2array(box, img_size):\n",
    "    xmin, xmax, ymin, ymax = prop2abs(box.center, box.size, img_size)\n",
    "    return np.array([xmin, xmax, ymin, ymax])\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def jaccard_overlap(box_arr, anchors_arr):\n",
    "    areaa = (anchors_arr[:, 1]-anchors_arr[:, 0]+1) * \\\n",
    "            (anchors_arr[:, 3]-anchors_arr[:, 2]+1)\n",
    "    areab = (box_arr[1]-box_arr[0]+1) * (box_arr[3]-box_arr[2]+1)\n",
    "\n",
    "    xxmin = np.maximum(box_arr[0], anchors_arr[:, 0])\n",
    "    xxmax = np.minimum(box_arr[1], anchors_arr[:, 1])\n",
    "    yymin = np.maximum(box_arr[2], anchors_arr[:, 2])\n",
    "    yymax = np.minimum(box_arr[3], anchors_arr[:, 3])\n",
    "\n",
    "    w = np.maximum(0, xxmax-xxmin+1)\n",
    "    h = np.maximum(0, yymax-yymin+1)\n",
    "    intersection = w*h\n",
    "    union = areab+areaa-intersection\n",
    "    return intersection/union\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def compute_overlap(box_arr, anchors_arr, threshold):\n",
    "    iou = jaccard_overlap(box_arr, anchors_arr)\n",
    "    overlap = iou > threshold\n",
    "\n",
    "    good_idxs = np.nonzero(overlap)[0]\n",
    "    best_idx  = np.argmax(iou)\n",
    "    best = None\n",
    "    good = []\n",
    "\n",
    "    if iou[best_idx] > threshold:\n",
    "        best = Score(best_idx, iou[best_idx])\n",
    "\n",
    "    for idx in good_idxs:\n",
    "        good.append(Score(idx, iou[idx]))\n",
    "\n",
    "    return Overlap(best, good)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def compute_location(box, anchor):\n",
    "    arr = np.zeros((4))\n",
    "    arr[0] = (box.center.x-anchor.center.x)/anchor.size.w*10\n",
    "    arr[1] = (box.center.y-anchor.center.y)/anchor.size.h*10\n",
    "    arr[2] = log(box.size.w/anchor.size.w)*5\n",
    "    arr[3] = log(box.size.h/anchor.size.h)*5\n",
    "    return arr\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def decode_location(box, anchor):\n",
    "    box[box > 100] = 100 # бывает только раннее обучение\n",
    "\n",
    "    x = box[0]/10 * anchor.size.w + anchor.center.x\n",
    "    y = box[1]/10 * anchor.size.h + anchor.center.y\n",
    "    w = exp(box[2]/5) * anchor.size.w\n",
    "    h = exp(box[3]/5) * anchor.size.h\n",
    "    return Point(x, y), Size(w, h)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def decode_boxes(pred, anchors, confidence_threshold = 0.01, lid2name = {},\n",
    "                 detections_cap=200):\n",
    "    \"\"\"\n",
    "    Decode boxes from the neural net predictions.\n",
    "    Label names are decoded using the lid2name dictionary - the id to name\n",
    "    translation is not done if the corresponding key does not exist.\n",
    "    \"\"\"\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Найдите обнаружения\n",
    "    #---------------------------------------------------------------------------\n",
    "    num_classes = pred.shape[1]-4\n",
    "    bg_class    = num_classes-1\n",
    "    box_class   = np.argmax(pred[:, :num_classes-1], axis=1)\n",
    "    confidence  = pred[np.arange(len(pred)), box_class]\n",
    "    if detections_cap is not None:\n",
    "        detections = np.argsort(confidence)[::-1][:detections_cap]\n",
    "    else:\n",
    "        detections = np.argsort(confidence)[::-1]\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Декодируйте координаты каждого ящика с уверенностью выше порога\n",
    "    #---------------------------------------------------------------------------\n",
    "    boxes = []\n",
    "    for idx in detections:\n",
    "        confidence = pred[idx, box_class[idx]]\n",
    "        if confidence < confidence_threshold:\n",
    "            break\n",
    "\n",
    "        center, size = decode_location(pred[idx, num_classes:], anchors[idx])\n",
    "        cid = box_class[idx]\n",
    "        cname = None\n",
    "        if cid in lid2name:\n",
    "            cname = lid2name[cid]\n",
    "        det = (confidence, normalize_box(Box(cname, cid, center, size)))\n",
    "        boxes.append(det)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def non_maximum_suppression(boxes, overlap_threshold):\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Преобразование в абсолютные координаты и в более удобный формат\n",
    "    #---------------------------------------------------------------------------\n",
    "    xmin = []\n",
    "    xmax = []\n",
    "    ymin = []\n",
    "    ymax = []\n",
    "    conf = []\n",
    "    img_size = Size(1000, 1000)\n",
    "\n",
    "    for box in boxes:\n",
    "        params = prop2abs(box[1].center, box[1].size, img_size)\n",
    "        xmin.append(params[0])\n",
    "        xmax.append(params[1])\n",
    "        ymin.append(params[2])\n",
    "        ymax.append(params[3])\n",
    "        conf.append(box[0])\n",
    "\n",
    "    xmin = np.array(xmin)\n",
    "    xmax = np.array(xmax)\n",
    "    ymin = np.array(ymin)\n",
    "    ymax = np.array(ymax)\n",
    "    conf = np.array(conf)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Вычислите площадь каждого ящика и отсортируйте индексы по уровню достоверности.\n",
    "    # (сначала самая низкая достоверность).\n",
    "    #---------------------------------------------------------------------------\n",
    "    area = (xmax-xmin+1) * (ymax-ymin+1)\n",
    "    idxs = np.argsort(conf)\n",
    "    pick = []\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Цикл, пока у нас не останутся индексы для обработки\n",
    "    #---------------------------------------------------------------------------\n",
    "    while len(idxs) > 0:\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Возьмите последний индекс (т.е. наиболее надежное обнаружение), удалите его из\n",
    "        # список индексов для обработки и помещаем в список пикировок\n",
    "        #-----------------------------------------------------------------------\n",
    "        last = idxs.shape[0]-1\n",
    "        i    = idxs[last]\n",
    "        idxs = np.delete(idxs, last)\n",
    "        pick.append(i)\n",
    "        suppress = []\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Выясните пересечение с оставшимися окнами\n",
    "        #-----------------------------------------------------------------------\n",
    "        xxmin = np.maximum(xmin[i], xmin[idxs])\n",
    "        xxmax = np.minimum(xmax[i], xmax[idxs])\n",
    "        yymin = np.maximum(ymin[i], ymin[idxs])\n",
    "        yymax = np.minimum(ymax[i], ymax[idxs])\n",
    "\n",
    "        w = np.maximum(0, xxmax-xxmin+1)\n",
    "        h = np.maximum(0, yymax-yymin+1)\n",
    "        intersection = w*h\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Вычислять IOU и подавлять индексы с IOU выше порогового значения\n",
    "        #-----------------------------------------------------------------------\n",
    "        union    = area[i]+area[idxs]-intersection\n",
    "        iou      = intersection/union\n",
    "        overlap  = iou > overlap_threshold\n",
    "        suppress = np.nonzero(overlap)[0]\n",
    "        idxs     = np.delete(idxs, suppress)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Вернуть выбранные ящики\n",
    "    #---------------------------------------------------------------------------\n",
    "    selected = []\n",
    "    for i in pick:\n",
    "        selected.append(boxes[i])\n",
    "\n",
    "    return selected\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def suppress_overlaps(boxes):\n",
    "    class_boxes    = defaultdict(list)\n",
    "    selected_boxes = []\n",
    "    for box in boxes:\n",
    "        class_boxes[box[1].labelid].append(box)\n",
    "\n",
    "    for k, v in class_boxes.items():\n",
    "        selected_boxes += non_maximum_suppression(v, 0.45)\n",
    "    return selected_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3324637-6a15-47c3-9962-b5227f9d8805",
   "metadata": {},
   "source": [
    "Usage\n",
    "\n",
    "Затем вам нужно предварительно обработать набор данных, прежде чем вы сможете обучить на нем модель. Можно использовать настройки по умолчанию, но если вы хотите чего-то большего, всегда полезно попробовать параметр --help.\n",
    "./process_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce0868b-514b-4dc4-9d37-721208ddadd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Data source:           pascal_voc\n",
      "[i] Data directory:        pascal-voc\n",
      "[i] Validation fraction:   0.025\n",
      "[i] Expand probability:    0.5\n",
      "[i] Sampler trials:        50\n",
      "[i] Annotate:              False\n",
      "[i] Compute training data: True\n",
      "[i] Preset:                vgg300\n",
      "[i] Process test dataset:  False\n",
      "[i] Configuring the data source...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainval_VOC2007: 100%|██████████| 5011/5011 [00:20<00:00, 249.31samples/s]\n",
      "trainval_VOC2012: 100%|██████████| 11540/11540 [00:53<00:00, 215.14samples/s]\n",
      "test_VOC2007: 100%|██████████| 4952/4952 [00:18<00:00, 273.67samples/s]\n",
      "valid_VOC2012: 100%|██████████| 17125/17125 [01:12<00:00, 237.01samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] # training samples:    21503\n",
      "[i] # validation samples:  17125\n",
      "[i] # testing samples:     0\n",
      "[i] # classes:             20\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Load process_dataset.py\n",
    "#-------------------------------------------------------------------------------\n",
    "# Значения аргументов со значениями по умолчанию\n",
    "DATA_SOURCE = 'pascal_voc'\n",
    "DATA_DIR = 'pascal-voc'\n",
    "VALIDATION_FRACTION = 0.025\n",
    "EXPAND_PROBABILITY = 0.5\n",
    "SAMPLER_TRIALS = 50\n",
    "ANNOTATE = False\n",
    "COMPUTE_TD = True\n",
    "PRESET = 'vgg300'\n",
    "PROCESS_TEST = False\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def annotate(data_dir, samples, colors, sample_name):\n",
    "    \"\"\"\n",
    "    Draw the bounding boxes on the sample images\n",
    "    :param data_dir: the directory where the dataset's files are stored\n",
    "    :param samples:  samples to be processed\n",
    "    :param colors:   a dictionary mapping class name to a BGR color tuple\n",
    "    :param colors:   name of the sample\n",
    "    \"\"\"\n",
    "    result_dir = data_dir + '/annotated/' + sample_name.strip() + '/'\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "\n",
    "    for sample in tqdm(samples, desc=sample_name, unit='samples'):\n",
    "        img = cv2.imread(sample.filename)\n",
    "        basefn = os.path.basename(sample.filename)\n",
    "        for box in sample.boxes:\n",
    "            draw_box(img, box, colors[box.label])\n",
    "        cv2.imwrite(result_dir + basefn, img)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def build_sampler(overlap, trials):\n",
    "    return SamplerTransform(sample=True, min_scale=0.3, max_scale=1.0,\n",
    "                            min_aspect_ratio=0.5, max_aspect_ratio=2.0,\n",
    "                            min_jaccard_overlap=overlap, max_trials=trials)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def build_train_transforms(preset, num_classes, sampler_trials, expand_prob):\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Изменение размера\n",
    "    #---------------------------------------------------------------------------\n",
    "    tf_resize = ResizeTransform(width=preset.image_size.w,\n",
    "                                height=preset.image_size.h,\n",
    "                                algorithms=[cv2.INTER_LINEAR,\n",
    "                                            cv2.INTER_AREA,\n",
    "                                            cv2.INTER_NEAREST,\n",
    "                                            cv2.INTER_CUBIC,\n",
    "                                            cv2.INTER_LANCZOS4])\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Искажения изображения\n",
    "    #---------------------------------------------------------------------------\n",
    "    tf_brightness = BrightnessTransform(delta=32)\n",
    "    tf_rnd_brightness = RandomTransform(prob=0.5, transform=tf_brightness)\n",
    "\n",
    "    tf_contrast = ContrastTransform(lower=0.5, upper=1.5)\n",
    "    tf_rnd_contrast = RandomTransform(prob=0.5, transform=tf_contrast)\n",
    "\n",
    "    tf_hue = HueTransform(delta=18)\n",
    "    tf_rnd_hue = RandomTransform(prob=0.5, transform=tf_hue)\n",
    "\n",
    "    tf_saturation = SaturationTransform(lower=0.5, upper=1.5)\n",
    "    tf_rnd_saturation = RandomTransform(prob=0.5, transform=tf_saturation)\n",
    "\n",
    "    tf_reorder_channels = ReorderChannelsTransform()\n",
    "    tf_rnd_reorder_channels = RandomTransform(prob=0.5,\n",
    "                                              transform=tf_reorder_channels)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Композиции искажений изображения\n",
    "    #---------------------------------------------------------------------------\n",
    "    tf_distort_lst = [\n",
    "        tf_rnd_contrast,\n",
    "        tf_rnd_saturation,\n",
    "        tf_rnd_hue,\n",
    "        tf_rnd_contrast\n",
    "    ]\n",
    "    tf_distort_1 = ComposeTransform(transforms=tf_distort_lst[:-1])\n",
    "    tf_distort_2 = ComposeTransform(transforms=tf_distort_lst[1:])\n",
    "    tf_distort_comp = [tf_distort_1, tf_distort_2]\n",
    "    tf_distort = TransformPickerTransform(transforms=tf_distort_comp)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Развернуть образец\n",
    "    #---------------------------------------------------------------------------\n",
    "    tf_expand = ExpandTransform(max_ratio=4.0, mean_value=[104, 117, 123])\n",
    "    tf_rnd_expand = RandomTransform(prob=expand_prob, transform=tf_expand)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Пробники\n",
    "    #---------------------------------------------------------------------------\n",
    "    samplers = [\n",
    "        SamplerTransform(sample=False),\n",
    "        build_sampler(0.1, sampler_trials),\n",
    "        build_sampler(0.3, sampler_trials),\n",
    "        build_sampler(0.5, sampler_trials),\n",
    "        build_sampler(0.7, sampler_trials),\n",
    "        build_sampler(0.9, sampler_trials),\n",
    "        build_sampler(1.0, sampler_trials)\n",
    "    ]\n",
    "    tf_sample_picker = SamplePickerTransform(samplers=samplers)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Горизонтальный флип\n",
    "    #---------------------------------------------------------------------------\n",
    "    tf_flip = HorizontalFlipTransform()\n",
    "    tf_rnd_flip = RandomTransform(prob=0.5, transform=tf_flip)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Преобразовать список\n",
    "    #---------------------------------------------------------------------------\n",
    "    transforms = [\n",
    "        ImageLoaderTransform(),\n",
    "        tf_rnd_brightness,\n",
    "        tf_distort,\n",
    "        tf_rnd_reorder_channels,\n",
    "        tf_rnd_expand,\n",
    "        tf_sample_picker,\n",
    "        tf_rnd_flip,\n",
    "        LabelCreatorTransform(preset=preset, num_classes=num_classes),\n",
    "        tf_resize\n",
    "    ]\n",
    "    return transforms\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def build_valid_transforms(preset, num_classes):\n",
    "    tf_resize = ResizeTransform(width=preset.image_size.w,\n",
    "                                height=preset.image_size.h,\n",
    "                                algorithms=[cv2.INTER_LINEAR])\n",
    "    transforms = [\n",
    "        ImageLoaderTransform(),\n",
    "        LabelCreatorTransform(preset=preset, num_classes=num_classes),\n",
    "        tf_resize\n",
    "    ]\n",
    "    return transforms\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def main():\n",
    "    print('[i] Data source:          ', DATA_SOURCE)\n",
    "    print('[i] Data directory:       ', DATA_DIR)\n",
    "    print('[i] Validation fraction:  ', VALIDATION_FRACTION)\n",
    "    print('[i] Expand probability:   ', EXPAND_PROBABILITY)\n",
    "    print('[i] Sampler trials:       ', SAMPLER_TRIALS)\n",
    "    print('[i] Annotate:             ', ANNOTATE)\n",
    "    print('[i] Compute training data:', COMPUTE_TD)\n",
    "    print('[i] Preset:               ', PRESET)\n",
    "    print('[i] Process test dataset: ', PROCESS_TEST)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Загрузите источник данных\n",
    "    #---------------------------------------------------------------------------\n",
    "    print('[i] Configuring the data source...')\n",
    "    try:\n",
    "        source = load_data_source(DATA_SOURCE)\n",
    "        source.load_trainval_data(DATA_DIR, VALIDATION_FRACTION)\n",
    "        if PROCESS_TEST:\n",
    "            source.load_test_data(DATA_DIR)\n",
    "        print('[i] # training samples:   ', source.num_train)\n",
    "        print('[i] # validation samples: ', source.num_valid)\n",
    "        print('[i] # testing samples:    ', source.num_test)\n",
    "        print('[i] # classes:            ', source.num_classes)\n",
    "    except (ImportError, AttributeError, RuntimeError) as e:\n",
    "        print('[!] Unable to load data source:', str(e))\n",
    "        return 1\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Аннотировать образцы\n",
    "    #---------------------------------------------------------------------------\n",
    "    if ANNOTATE:\n",
    "        print('[i] Annotating samples...')\n",
    "        annotate(DATA_DIR, source.train_samples, source.colors, 'train')\n",
    "        annotate(DATA_DIR, source.valid_samples, source.colors, 'valid')\n",
    "        if PROCESS_TEST:\n",
    "            annotate(DATA_DIR, source.test_samples,  source.colors, 'test ')\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Вычислите обучающие данные\n",
    "    #---------------------------------------------------------------------------\n",
    "    if COMPUTE_TD:\n",
    "        preset = get_preset_by_name(PRESET)\n",
    "        with open(DATA_DIR+'/train-samples.pkl', 'wb') as f:\n",
    "            pickle.dump(source.train_samples, f)\n",
    "        with open(DATA_DIR+'/valid-samples.pkl', 'wb') as f:\n",
    "            pickle.dump(source.valid_samples, f)\n",
    "\n",
    "        with open(DATA_DIR+'/training-data.pkl', 'wb') as f:\n",
    "            data = {\n",
    "                'preset': preset,\n",
    "                'num-classes': source.num_classes,\n",
    "                'colors': source.colors,\n",
    "                'lid2name': source.lid2name,\n",
    "                'lname2id': source.lname2id,\n",
    "                'train-transforms': build_train_transforms(preset,\n",
    "                                       source.num_classes, SAMPLER_TRIALS,\n",
    "                                       EXPAND_PROBABILITY),\n",
    "                'valid-transforms': build_valid_transforms(preset,\n",
    "                                                           source.num_classes)\n",
    "            }\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc1785-7bd9-44cb-9279-b64bf5138d8e",
   "metadata": {},
   "source": [
    "!!! код выполнен успешно, предупреждение «SystemExit: 0» типично при запуске скрипта Python из IPython или Jupyter Notebook. Это предупреждение можно игнорировать.\n",
    "\n",
    "Скрипт обработал источник данных и предоставил некоторую полезную информацию,\n",
    "такую как количество обучающих и проверочных образцов, а также количество классов.\\\n",
    "Это также указывает на то, что аннотация и вычисление обучающих данных не выполнялись, что соответствует значениям переменных ANNOTATE и COMPUTE_TD в начале скрипта.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9f011-bc02-4e04-a209-aba7b17ee5ab",
   "metadata": {},
   "source": [
    "Usage\n",
    "\n",
    "Затем вы можете тренировать все это. Для получения хороших результатов потребуется от 150 до 200 эпох. Опять же, вы можете попробовать --help, если хотите сделать что-то нестандартное.\n",
    "./train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec629350-06ff-4fd2-adcc-c9e9b8d30ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Project name:          test\n",
      "[i] Data directory:        pascal-voc\n",
      "[i] VGG directory:         vgg_graph\n",
      "[i] # epochs:              1\n",
      "[i] Batch size:            3\n",
      "[i] Tensorboard directory: tb\n",
      "[i] Checkpoint interval:   5\n",
      "[i] Learning rate values:  0.00075;0.0001;0.00001\n",
      "[i] Learning rate boundaries:  320000;400000\n",
      "[i] Momentum:              0.9\n",
      "[i] Weight decay:          0.0005\n",
      "[i] Continue:              False\n",
      "[i] Number of workers:     8\n",
      "[i] Creating directory test...\n",
      "[i] Starting at epoch:     1\n",
      "[i] Configuring the training data...\n",
      "[i] # training samples:    21503\n",
      "[i] # validation samples:  17125\n",
      "[i] # classes:             20\n",
      "[i] Image size:            Size(w=300, h=300)\n",
      "[i] Creating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "997MB [03:18, 5.01MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 291\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 291\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[32], line 140\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    138\u001b[0m     net\u001b[38;5;241m.\u001b[39mbuild_optimizer_from_metagraph()\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     net\u001b[38;5;241m.\u001b[39mbuild_from_vgg(args\u001b[38;5;241m.\u001b[39mvgg_dir, td\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[0;32m    141\u001b[0m     net\u001b[38;5;241m.\u001b[39mbuild_optimizer(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m    142\u001b[0m                         global_step\u001b[38;5;241m=\u001b[39mglobal_step,\n\u001b[0;32m    143\u001b[0m                         weight_decay\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mweight_decay,\n\u001b[0;32m    144\u001b[0m                         momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m#-----------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# оздание различных помощников\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m#-----------------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32m~\\000 Start to Neural Networks\\Lesson_7\\ssdvgg.py:110\u001b[0m, in \u001b[0;36mSSDVGG.build_from_vgg\u001b[1;34m(self, vgg_dir, num_classes, a_trous, progress_hook)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__download_vgg(vgg_dir, progress_hook)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load_vgg(vgg_dir)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a_trous: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_vgg_mods_a_trous()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_vgg_mods()\n",
      "File \u001b[1;32m~\\000 Start to Neural Networks\\Lesson_7\\ssdvgg.py:192\u001b[0m, in \u001b[0;36mSSDVGG.__load_vgg\u001b[1;34m(self, vgg_dir)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__load_vgg\u001b[39m(\u001b[38;5;28mself\u001b[39m, vgg_dir):\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# Загрузите модель VGG, используя tf.keras.models.load_model.\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(vgg_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/vgg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Получить необходимые тензоры из загруженной модели\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_input \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minput\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    231\u001b[0m         filepath,\n\u001b[0;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    235\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    239\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    240\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:253\u001b[0m, in \u001b[0;36m_generate_object_paths\u001b[1;34m(object_graph_def)\u001b[0m\n\u001b[0;32m    251\u001b[0m current_node \u001b[38;5;241m=\u001b[39m nodes_to_visit\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    252\u001b[0m current_path \u001b[38;5;241m=\u001b[39m paths[current_node]\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reference \u001b[38;5;129;01min\u001b[39;00m object_graph_def\u001b[38;5;241m.\u001b[39mnodes[current_node]\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reference\u001b[38;5;241m.\u001b[39mnode_id \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Step 12: Load train.py\n",
    "#-------------------------------------------------------------------------------\n",
    "import argparse\n",
    "\n",
    "# Функция для вычисления скорости обучения\n",
    "def compute_lr(lr_values, lr_boundaries):\n",
    "    with tf.compat.v1.variable_scope('learning_rate'):\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step', dtype=tf.int64)\n",
    "        lr = tf.compat.v1.train.piecewise_constant(global_step, lr_boundaries, lr_values)\n",
    "    return lr, global_step\n",
    "#-------------------------------------------------------------------------------\n",
    "def main():\n",
    "    # Arguments (вручную установить значения здесь)\n",
    "    args = argparse.Namespace()\n",
    "    args.name = 'test'\n",
    "    args.data_dir = 'pascal-voc'\n",
    "    args.vgg_dir = 'vgg_graph'\n",
    "    args.epochs = 1\n",
    "    args.batch_size = 3\n",
    "    args.tensorboard_dir = 'tb'\n",
    "    args.checkpoint_interval = 5\n",
    "    args.lr_values = '0.00075;0.0001;0.00001'\n",
    "    args.lr_boundaries = '320000;400000'\n",
    "    args.momentum = 0.9\n",
    "    args.weight_decay = 0.0005\n",
    "    args.continue_training = False\n",
    "    args.num_workers = mp.cpu_count()\n",
    "    \n",
    "    print('[i] Project name:         ', args.name)\n",
    "    print('[i] Data directory:       ', args.data_dir)\n",
    "    print('[i] VGG directory:        ', args.vgg_dir)\n",
    "    print('[i] # epochs:             ', args.epochs)\n",
    "    print('[i] Batch size:           ', args.batch_size)\n",
    "    print('[i] Tensorboard directory:', args.tensorboard_dir)\n",
    "    print('[i] Checkpoint interval:  ', args.checkpoint_interval)\n",
    "    print('[i] Learning rate values: ', args.lr_values)\n",
    "    print('[i] Learning rate boundaries: ', args.lr_boundaries)\n",
    "    print('[i] Momentum:             ', args.momentum)\n",
    "    print('[i] Weight decay:         ', args.weight_decay)\n",
    "    print('[i] Continue:             ', args.continue_training)\n",
    "    print('[i] Number of workers:    ', args.num_workers)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Найти существующую контрольную точку\n",
    "    #---------------------------------------------------------------------------\n",
    "    start_epoch = 0\n",
    "    if args.continue_training:\n",
    "        state = tf.train.get_checkpoint_state(args.name)\n",
    "        if state is None:\n",
    "            print('[!] No network state found in ' + args.name)\n",
    "            return 1\n",
    "\n",
    "        ckpt_paths = state.all_model_checkpoint_paths\n",
    "        if not ckpt_paths:\n",
    "            print('[!] No network state found in ' + args.name)\n",
    "            return 1\n",
    "\n",
    "        last_epoch = None\n",
    "        checkpoint_file = None\n",
    "        for ckpt in ckpt_paths:\n",
    "            ckpt_num = os.path.basename(ckpt).split('.')[0][1:]\n",
    "            try:\n",
    "                ckpt_num = int(ckpt_num)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if last_epoch is None or last_epoch < ckpt_num:\n",
    "                last_epoch = ckpt_num\n",
    "                checkpoint_file = ckpt\n",
    "\n",
    "        if checkpoint_file is None:\n",
    "            print('[!] No checkpoints found, cannot continue!')\n",
    "            return 1\n",
    "\n",
    "        metagraph_file = checkpoint_file + '.meta'\n",
    "\n",
    "        if not os.path.exists(metagraph_file):\n",
    "            print('[!] Cannot find metagraph', metagraph_file)\n",
    "            return 1\n",
    "        start_epoch = last_epoch\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Создать каталог проекта\n",
    "    #---------------------------------------------------------------------------\n",
    "    else:\n",
    "        try:\n",
    "            print('[i] Creating directory {}...'.format(args.name))\n",
    "            os.makedirs(args.name)\n",
    "        except (IOError) as e:\n",
    "            print('[!]', str(e))\n",
    "            return 1\n",
    "\n",
    "    print('[i] Starting at epoch:    ', start_epoch+1)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Настройте данные обучения\n",
    "    #---------------------------------------------------------------------------\n",
    "    print('[i] Configuring the training data...')\n",
    "    try:\n",
    "        td = TrainingData(args.data_dir)\n",
    "        print('[i] # training samples:   ', td.num_train)\n",
    "        print('[i] # validation samples: ', td.num_valid)\n",
    "        print('[i] # classes:            ', td.num_classes)\n",
    "        print('[i] Image size:           ', td.preset.image_size)\n",
    "    except (AttributeError, RuntimeError) as e:\n",
    "        print('[!] Unable to load training data:', str(e))\n",
    "        return 1\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Создайте сеть\n",
    "    #---------------------------------------------------------------------------\n",
    "    with tf.compat.v1.Session() as sess:  # Use tf.compat.v1.Session for compatibility\n",
    "        print('[i] Creating the model...')\n",
    "        n_train_batches = int(math.ceil(td.num_train / args.batch_size))\n",
    "        n_valid_batches = int(math.ceil(td.num_valid / args.batch_size))\n",
    "\n",
    "        global_step = None\n",
    "        if start_epoch == 0:\n",
    "            lr_values = args.lr_values.split(';')\n",
    "            try:\n",
    "                lr_values = [float(x) for x in lr_values]\n",
    "            except ValueError:\n",
    "                print('[!] Learning rate values must be floats')\n",
    "                sys.exit(1)\n",
    "\n",
    "            lr_boundaries = args.lr_boundaries.split(';')\n",
    "            try:\n",
    "                lr_boundaries = [int(x) for x in lr_boundaries]\n",
    "            except ValueError:\n",
    "                print('[!] Learning rate boundaries must be ints')\n",
    "                sys.exit(1)\n",
    "\n",
    "            ret = compute_lr(lr_values, lr_boundaries)\n",
    "            learning_rate, global_step = ret\n",
    "\n",
    "        net = SSDVGG(sess, td.preset)\n",
    "        if start_epoch != 0:\n",
    "            net.build_from_metagraph(metagraph_file, checkpoint_file)\n",
    "            net.build_optimizer_from_metagraph()\n",
    "        else:\n",
    "            net.build_from_vgg(args.vgg_dir, td.num_classes)\n",
    "            net.build_optimizer(learning_rate=learning_rate,\n",
    "                                global_step=global_step,\n",
    "                                weight_decay=args.weight_decay,\n",
    "                                momentum=args.momentum)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # оздание различных помощников\n",
    "        #-----------------------------------------------------------------------\n",
    "        summary_writer = tf.summary.FileWriter(args.tensorboard_dir,\n",
    "                                               sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=20)\n",
    "\n",
    "        anchors = get_anchors_for_preset(td.preset)\n",
    "        training_ap_calc = APCalculator()\n",
    "        validation_ap_calc = APCalculator()\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Резюме\n",
    "        #-----------------------------------------------------------------------\n",
    "        restore = start_epoch != 0\n",
    "\n",
    "        training_ap = PrecisionSummary(sess, summary_writer, 'training',\n",
    "                                       td.lname2id.keys(), restore)\n",
    "        validation_ap = PrecisionSummary(sess, summary_writer, 'validation',\n",
    "                                         td.lname2id.keys(), restore)\n",
    "\n",
    "        training_imgs = ImageSummary(sess, summary_writer, 'training',\n",
    "                                     td.label_colors, restore)\n",
    "        validation_imgs = ImageSummary(sess, summary_writer, 'validation',\n",
    "                                       td.label_colors, restore)\n",
    "\n",
    "        training_loss = LossSummary(sess, summary_writer, 'training',\n",
    "                                    td.num_train, restore)\n",
    "        validation_loss = LossSummary(sess, summary_writer, 'validation',\n",
    "                                      td.num_valid, restore)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Get the initial snapshot of the network\n",
    "        #-----------------------------------------------------------------------\n",
    "        net_summary_ops = net.build_summaries(restore)\n",
    "        if start_epoch == 0:\n",
    "            net_summary = sess.run(net_summary_ops)\n",
    "            summary_writer.add_summary(net_summary, 0)\n",
    "        summary_writer.flush()\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Cycle through the epoch\n",
    "        #-----------------------------------------------------------------------\n",
    "        print('[i] Training...')\n",
    "        for e in range(start_epoch, args.epochs):\n",
    "            training_imgs_samples = []\n",
    "            validation_imgs_samples = []\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Train\n",
    "            #-------------------------------------------------------------------\n",
    "            generator = td.train_generator(args.batch_size, args.num_workers)\n",
    "            description = '[i] Train {:>2}/{}'.format(e+1, args.epochs)\n",
    "            for x, y, gt_boxes in tqdm(generator, total=n_train_batches,\n",
    "                                       desc=description, unit='batches'):\n",
    "\n",
    "                if len(training_imgs_samples) < 3:\n",
    "                    saved_images = np.copy(x[:3])\n",
    "\n",
    "                feed = {net.image_input: x,\n",
    "                        net.labels: y}\n",
    "                result, loss_batch, _ = sess.run([net.result, net.losses,\n",
    "                                                  net.optimizer],\n",
    "                                                 feed_dict=feed)\n",
    "\n",
    "                if math.isnan(loss_batch['confidence']):\n",
    "                    print('[!] Confidence loss is NaN.')\n",
    "\n",
    "                training_loss.add(loss_batch, x.shape[0])\n",
    "\n",
    "                if e == 0: continue\n",
    "\n",
    "                for i in range(result.shape[0]):\n",
    "                    boxes = decode_boxes(result[i], anchors, 0.5, td.lid2name)\n",
    "                    boxes = suppress_overlaps(boxes)\n",
    "                    training_ap_calc.add_detections(gt_boxes[i], boxes)\n",
    "\n",
    "                    if len(training_imgs_samples) < 3:\n",
    "                        training_imgs_samples.append((saved_images[i], boxes))\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Validate\n",
    "            #-------------------------------------------------------------------\n",
    "            generator = td.valid_generator(args.batch_size, args.num_workers)\n",
    "            description = '[i] Valid {:>2}/{}'.format(e+1, args.epochs)\n",
    "\n",
    "            for x, y, gt_boxes in tqdm(generator, total=n_valid_batches,\n",
    "                                       desc=description, unit='batches'):\n",
    "                feed = {net.image_input: x,\n",
    "                        net.labels: y}\n",
    "                result, loss_batch = sess.run([net.result, net.losses],\n",
    "                                              feed_dict=feed)\n",
    "\n",
    "                validation_loss.add(loss_batch,  x.shape[0])\n",
    "\n",
    "                if e == 0: continue\n",
    "\n",
    "                for i in range(result.shape[0]):\n",
    "                    boxes = decode_boxes(result[i], anchors, 0.5, td.lid2name)\n",
    "                    boxes = suppress_overlaps(boxes)\n",
    "                    validation_ap_calc.add_detections(gt_boxes[i], boxes)\n",
    "\n",
    "                    if len(validation_imgs_samples) < 3:\n",
    "                        validation_imgs_samples.append((np.copy(x[i]), boxes))\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Write summaries\n",
    "            #-------------------------------------------------------------------\n",
    "            training_loss.push(e+1)\n",
    "            validation_loss.push(e+1)\n",
    "\n",
    "            net_summary = sess.run(net_summary_ops)\n",
    "            summary_writer.add_summary(net_summary, e+1)\n",
    "\n",
    "            APs = training_ap_calc.compute_aps()\n",
    "            mAP = APs2mAP(APs)\n",
    "            training_ap.push(e+1, mAP, APs)\n",
    "\n",
    "            APs = validation_ap_calc.compute_aps()\n",
    "            mAP = APs2mAP(APs)\n",
    "            validation_ap.push(e+1, mAP, APs)\n",
    "\n",
    "            training_ap_calc.clear()\n",
    "            validation_ap_calc.clear()\n",
    "\n",
    "            training_imgs.push(e+1, training_imgs_samples)\n",
    "            validation_imgs.push(e+1, validation_imgs_samples)\n",
    "\n",
    "            summary_writer.flush()\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Save a checktpoint\n",
    "            #-------------------------------------------------------------------\n",
    "            if (e+1) % args.checkpoint_interval == 0:\n",
    "                checkpoint = '{}/e{}.ckpt'.format(args.name, e+1)\n",
    "                saver.save(sess, checkpoint)\n",
    "                print('[i] Checkpoint saved:', checkpoint)\n",
    "\n",
    "        checkpoint = '{}/final.ckpt'.format(args.name)\n",
    "        saver.save(sess, checkpoint)\n",
    "        print('[i] Checkpoint saved:', checkpoint)\n",
    "\n",
    "    return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b26ee0-c8b1-4b6b-a17c-cf4d054b6785",
   "metadata": {},
   "source": [
    "Usage\n",
    "\n",
    "Вы можете аннотировать изображения, делать дамп необработанных прогнозов, распечатывать статистику AP или экспортировать результаты в формат, совместимый с Pascal VOC, с помощью сценария вывода.\n",
    "./infer.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84767af3-ca26-4c9e-9a68-06b6fdcce2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Load infer.py\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import math\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from average_precision import APCalculator, APs2mAP\n",
    "from pascal_summary import PascalSummary\n",
    "from ssdutils import get_anchors_for_preset, decode_boxes, suppress_overlaps\n",
    "from ssdvgg import SSDVGG\n",
    "from utils import str2bool, load_data_source, draw_box\n",
    "from tqdm import tqdm\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    print(\"This is a Python 3 program. Use Python 3 or higher.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def sample_generator(samples, image_size, batch_size):\n",
    "    image_size = (image_size.w, image_size.h)\n",
    "    for offset in range(0, len(samples), batch_size):\n",
    "        files = samples[offset:offset+batch_size]\n",
    "        images = []\n",
    "        idxs   = []\n",
    "        for i, image_file in enumerate(files):\n",
    "            image = cv2.resize(cv2.imread(image_file), image_size)\n",
    "            images.append(image.astype(np.float32))\n",
    "            idxs.append(offset+i)\n",
    "        yield np.array(images), idxs\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def main():\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Разобрать командную строку\n",
    "    #---------------------------------------------------------------------------\n",
    "    parser = argparse.ArgumentParser(description='SSD inference')\n",
    "    parser.add_argument(\"files\", nargs=\"*\")\n",
    "    parser.add_argument('--name', default='test',\n",
    "                        help='project name')\n",
    "    parser.add_argument('--checkpoint', type=int, default=-1,\n",
    "                        help='checkpoint to restore; -1 is the most recent')\n",
    "    parser.add_argument('--training-data',\n",
    "                        default='pascal-voc/training-data.pkl',\n",
    "                        help='Information about parameters used for training')\n",
    "    parser.add_argument('--output-dir', default='test-output',\n",
    "                        help='directory for the resulting images')\n",
    "    parser.add_argument('--annotate', type=str2bool, default='False',\n",
    "                        help=\"Annotate the data samples\")\n",
    "    parser.add_argument('--dump-predictions', type=str2bool, default='False',\n",
    "                        help=\"Dump raw predictions\")\n",
    "    parser.add_argument('--compute-stats', type=str2bool, default='True',\n",
    "                        help=\"Compute the mAP stats\")\n",
    "    parser.add_argument('--data-source', default=None,\n",
    "                        help='Use test files from the data source')\n",
    "    parser.add_argument('--data-dir', default='pascal-voc',\n",
    "                        help='Use test files from the data source')\n",
    "    parser.add_argument('--batch-size', type=int, default=32,\n",
    "                        help='batch size')\n",
    "    parser.add_argument('--sample', default='test',\n",
    "                        choices=['test', 'trainval'], help='sample to run on')\n",
    "    parser.add_argument('--threshold', type=float, default=0.5,\n",
    "                        help='confidence threshold')\n",
    "    parser.add_argument('--pascal-summary', type=str2bool, default='False',\n",
    "                        help='dump the detections in Pascal VOC format')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Параметры печати\n",
    "    #---------------------------------------------------------------------------\n",
    "    print('[i] Project name:      ', args.name)\n",
    "    print('[i] Training data:     ', args.training_data)\n",
    "    print('[i] Batch size:        ', args.batch_size)\n",
    "    print('[i] Data source:       ', args.data_source)\n",
    "    print('[i] Data directory:    ', args.data_dir)\n",
    "    print('[i] Output directory:  ', args.output_dir)\n",
    "    print('[i] Annotate:          ', args.annotate)\n",
    "    print('[i] Dump predictions:  ', args.dump_predictions)\n",
    "    print('[i] Sample:            ', args.sample)\n",
    "    print('[i] Threshold:         ', args.threshold)\n",
    "    print('[i] Pascal summary:    ', args.pascal_summary)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Проверьте, можем ли мы получить контрольно-пропускной пункт\n",
    "    #---------------------------------------------------------------------------\n",
    "    state = tf.train.get_checkpoint_state(args.name)\n",
    "    if state is None:\n",
    "        print('[!] No network state found in ' + args.name)\n",
    "        return 1\n",
    "\n",
    "    try:\n",
    "        checkpoint_file = state.all_model_checkpoint_paths[args.checkpoint]\n",
    "    except IndexError:\n",
    "        print('[!] Cannot find checkpoint ' + str(args.checkpoint_file))\n",
    "        return 1\n",
    "\n",
    "    metagraph_file = checkpoint_file + '.meta'\n",
    "\n",
    "    if not os.path.exists(metagraph_file):\n",
    "        print('[!] Cannot find metagraph ' + metagraph_file)\n",
    "        return 1\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Загрузите тренировочные данные\n",
    "    #---------------------------------------------------------------------------\n",
    "    try:\n",
    "        with open(args.training_data, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        preset = data['preset']\n",
    "        colors = data['colors']\n",
    "        lid2name = data['lid2name']\n",
    "        num_classes = data['num-classes']\n",
    "        image_size = preset.image_size\n",
    "        anchors = get_anchors_for_preset(preset)\n",
    "    except (FileNotFoundError, IOError, KeyError) as e:\n",
    "        print('[!] Unable to load training data:', str(e))\n",
    "        return 1\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Загрузите источник данных, если он определен\n",
    "    #---------------------------------------------------------------------------\n",
    "    compute_stats = False\n",
    "    source = None\n",
    "    if args.data_source:\n",
    "        print('[i] Configuring the data source...')\n",
    "        try:\n",
    "            source = load_data_source(args.data_source)\n",
    "            if args.sample == 'test':\n",
    "                source.load_test_data(args.data_dir)\n",
    "                num_samples = source.num_test\n",
    "                samples     = source.test_samples\n",
    "            else:\n",
    "                source.load_trainval_data(args.data_dir, 0)\n",
    "                num_samples = source.num_train\n",
    "                samples = source.train_samples\n",
    "            print('[i] # samples:         ', num_samples)\n",
    "            print('[i] # classes:         ', source.num_classes)\n",
    "        except (ImportError, AttributeError, RuntimeError) as e:\n",
    "            print('[!] Unable to load data source:', str(e))\n",
    "            return 1\n",
    "\n",
    "        if args.compute_stats:\n",
    "            compute_stats = True\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Создайте список файлов для анализа и убедитесь, что выходной каталог\n",
    "    # существует\n",
    "    #---------------------------------------------------------------------------\n",
    "    files = []\n",
    "\n",
    "    if source:\n",
    "        for sample in samples:\n",
    "            files.append(sample.filename)\n",
    "\n",
    "    if not source:\n",
    "        if args.files:\n",
    "            files = args.files\n",
    "\n",
    "        if not files:\n",
    "            print('[!] No files specified')\n",
    "            return 1\n",
    "\n",
    "    files = list(filter(lambda x: os.path.exists(x), files))\n",
    "    if files:\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.makedirs(args.output_dir)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Распечатать статистику модели и набора данных\n",
    "    #---------------------------------------------------------------------------\n",
    "    print('[i] Compute stats:     ', compute_stats)\n",
    "    print('[i] Network checkpoint:', checkpoint_file)\n",
    "    print('[i] Metagraph file:    ', metagraph_file)\n",
    "    print('[i] Image size:        ', image_size)\n",
    "    print('[i] Number of files:   ', len(files))\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Создайте сеть\n",
    "    #---------------------------------------------------------------------------\n",
    "    if compute_stats:\n",
    "        ap_calc = APCalculator()\n",
    "\n",
    "    if args.pascal_summary:\n",
    "        pascal_summary = PascalSummary()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print('[i] Creating the model...')\n",
    "        net = SSDVGG(sess, preset)\n",
    "        net.build_from_metagraph(metagraph_file, checkpoint_file)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        # Обработать изображения\n",
    "        #-----------------------------------------------------------------------\n",
    "        generator = sample_generator(files, image_size, args.batch_size)\n",
    "        n_sample_batches = int(math.ceil(len(files)/args.batch_size))\n",
    "        description = '[i] Processing samples'\n",
    "\n",
    "        for x, idxs in tqdm(generator, total=n_sample_batches,\n",
    "                      desc=description, unit='batches'):\n",
    "            feed = {net.image_input:  x,\n",
    "                    net.keep_prob:    1}\n",
    "            enc_boxes = sess.run(net.result, feed_dict=feed)\n",
    "\n",
    "            #-------------------------------------------------------------------\n",
    "            # Обработайте прогнозы\n",
    "            #-------------------------------------------------------------------\n",
    "            for i in range(enc_boxes.shape[0]):\n",
    "                boxes = decode_boxes(enc_boxes[i], anchors, args.threshold,\n",
    "                                     lid2name, None)\n",
    "                boxes = suppress_overlaps(boxes)[:200]\n",
    "                filename = files[idxs[i]]\n",
    "                basename = os.path.basename(filename)\n",
    "\n",
    "                #---------------------------------------------------------------\n",
    "                # Аннотировать образцы\n",
    "                #---------------------------------------------------------------\n",
    "                if args.annotate:\n",
    "                    img = cv2.imread(filename)\n",
    "                    for box in boxes:\n",
    "                        draw_box(img, box[1], colors[box[1].label])\n",
    "                    fn = args.output_dir+'/'+basename\n",
    "                    cv2.imwrite(fn, img)\n",
    "\n",
    "                #---------------------------------------------------------------\n",
    "                # Сбросить прогнозы\n",
    "                #---------------------------------------------------------------\n",
    "                if args.dump_predictions:\n",
    "                    raw_fn = args.output_dir+'/'+basename+'.npy'\n",
    "                    np.save(raw_fn, enc_boxes[i])\n",
    "\n",
    "                #---------------------------------------------------------------\n",
    "                # Добавляйте прогнозы в калькулятор статистики и в Паскаль\n",
    "                # краткое содержание\n",
    "                #---------------------------------------------------------------\n",
    "                if compute_stats:\n",
    "                    ap_calc.add_detections(samples[idxs[i]].boxes, boxes)\n",
    "\n",
    "                if args.pascal_summary:\n",
    "                    pascal_summary.add_detections(filename, boxes)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Подсчитайте и распечатайте статистику\n",
    "    #---------------------------------------------------------------------------\n",
    "    if compute_stats:\n",
    "        aps = ap_calc.compute_aps()\n",
    "        for k, v in aps.items():\n",
    "            print('[i] AP [{0}]: {1:.3f}'.format(k, v))\n",
    "        print('[i] mAP: {0:.3f}'.format(APs2mAP(aps)))\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Напишите сводные файлы Pascal\n",
    "    #---------------------------------------------------------------------------\n",
    "    if args.pascal_summary:\n",
    "        pascal_summary.write_summary(args.output_dir)\n",
    "\n",
    "    print('[i] All done.')\n",
    "    return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc7fd1-a93d-47b7-9a8c-28eca6caa6f6",
   "metadata": {},
   "source": [
    "Usage\n",
    "\n",
    "Чтобы экспортировать модель в график оптимизации вывода, запустите (используйте результат/результат в качестве имени выходного тензора):\n",
    "./export_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8a8ce-bc84-414a-bb1f-fa5790cb99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    print(\"This is a Python 3 program. Use Python 3 or higher.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# Разобрать командную строку\n",
    "#---------------------------------------------------------------------------\n",
    "parser = argparse.ArgumentParser(description='Export a tensorflow model')\n",
    "parser.add_argument('--metagraph-file', default='final.ckpt.meta',\n",
    "                    help='name of the metagraph file')\n",
    "parser.add_argument('--checkpoint-file', default='final.ckpt',\n",
    "                    help='name of the checkpoint file')\n",
    "parser.add_argument('--output-file', default='model.pb',\n",
    "                    help='name of the output file')\n",
    "parser.add_argument('--output-tensors', nargs='+',\n",
    "                    required=True,\n",
    "                    help='names of the output tensors')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('[i] Matagraph file:  ', args.metagraph_file)\n",
    "print('[i] Checkpoint file: ', args.checkpoint_file)\n",
    "print('[i] Output file:     ', args.output_file)\n",
    "print('[i] Output tensors:  ', args.output_tensors)\n",
    "\n",
    "for f in [args.checkpoint_file+'.index', args.metagraph_file]:\n",
    "    if not os.path.exists(f):\n",
    "        print('[!] Cannot find file:', f)\n",
    "        sys.exit(1)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Экспорт графика\n",
    "#-------------------------------------------------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(args.metagraph_file)\n",
    "    saver.restore(sess, args.checkpoint_file)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    input_graph_def = graph.as_graph_def()\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess, input_graph_def, args.output_tensors)\n",
    "\n",
    "    with open(args.output_file, \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21e52f-1ac4-4df7-802e-366d7f5d377f",
   "metadata": {},
   "source": [
    "Usage\n",
    "\n",
    "Если вы хотите сделать обнаружение на основе модели вывода, проверьте:\n",
    "./detect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d3c28e-c1be-4201-8ee5-380363f36bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from ssdutils import get_anchors_for_preset, decode_boxes, suppress_overlaps\n",
    "from utils import draw_box\n",
    "from tqdm import tqdm\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    print(\"This is a Python 3 program. Use Python 3 or higher.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Начать шоу\n",
    "#-------------------------------------------------------------------------------\n",
    "def main():\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Разобрать командную строку\n",
    "    #---------------------------------------------------------------------------\n",
    "    parser = argparse.ArgumentParser(description='SSD inference')\n",
    "    parser.add_argument(\"files\", nargs=\"*\")\n",
    "    parser.add_argument('--model', default='model300.pb',\n",
    "                        help='model file')\n",
    "    parser.add_argument('--training-data', default='training-data-300.pkl',\n",
    "                        help='training data')\n",
    "    parser.add_argument('--output-dir', default='test-out',\n",
    "                        help='output directory')\n",
    "    parser.add_argument('--batch-size', type=int, default=32,\n",
    "                        help='batch size')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Print parameters\n",
    "    #---------------------------------------------------------------------------\n",
    "    print('[i] Model:         ', args.model)\n",
    "    print('[i] Training data: ', args.training_data)\n",
    "    print('[i] Output dir:    ', args.output_dir)\n",
    "    print('[i] Batch size:    ', args.batch_size)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Загрузите график и данные обучения\n",
    "    #---------------------------------------------------------------------------\n",
    "    graph_def = tf.GraphDef()\n",
    "    with open(args.model, 'rb') as f:\n",
    "        serialized = f.read()\n",
    "        graph_def.ParseFromString(serialized)\n",
    "\n",
    "    with open(args.training_data, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        preset = data['preset']\n",
    "        colors = data['colors']\n",
    "        lid2name = data['lid2name']\n",
    "        anchors = get_anchors_for_preset(preset)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Создайте выходной каталог\n",
    "    #---------------------------------------------------------------------------\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Запускайте обнаружения пакетами\n",
    "    #---------------------------------------------------------------------------\n",
    "    with tf.Session() as sess:\n",
    "        tf.import_graph_def(graph_def, name='detector')\n",
    "        img_input = sess.graph.get_tensor_by_name('detector/image_input:0')\n",
    "        result = sess.graph.get_tensor_by_name('detector/result/result:0')\n",
    "\n",
    "        files = sys.argv[1:]\n",
    "\n",
    "        for i in tqdm(range(0, len(files), args.batch_size)):\n",
    "            batch_names = files[i:i+args.batch_size]\n",
    "            batch_imgs = []\n",
    "            batch = []\n",
    "            for f in batch_names:\n",
    "                img = cv2.imread(f)\n",
    "                batch_imgs.append(img)\n",
    "                img = cv2.resize(img, (300, 300))\n",
    "                batch.append(img)\n",
    "\n",
    "            batch = np.array(batch)\n",
    "            feed = {img_input: batch}\n",
    "            enc_boxes = sess.run(result, feed_dict=feed)\n",
    "\n",
    "            for i in range(len(batch_names)):\n",
    "                boxes = decode_boxes(enc_boxes[i], anchors, 0.5, lid2name, None)\n",
    "                boxes = suppress_overlaps(boxes)[:200]\n",
    "                name = os.path.basename(batch_names[i])\n",
    "\n",
    "                with open(os.path.join(args.output_dir, name+'.txt'), 'w') as f:\n",
    "                    for box in boxes:\n",
    "                        draw_box(batch_imgs[i], box[1], colors[box[1].label])\n",
    "\n",
    "                        box_data = '{} {} {} {} {} {}\\n'.format(box[1].label,\n",
    "                            box[1].labelid, box[1].center.x, box[1].center.y,\n",
    "                            box[1].size.w, box[1].size.h)\n",
    "                        f.write(box_data)\n",
    "\n",
    "                cv2.imwrite(os.path.join(args.output_dir, name),\n",
    "                            batch_imgs[i])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "549045ec-a5fd-450a-b17b-13254616ee1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './trainval/VOCdevkit//trainval/VOCdevkit/VOC2007/ImageSets/Main/trainval.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ground_truth)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 29\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[35], line 18\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m voc_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./trainval/VOCdevkit/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update this path to the correct directory\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load train and test datasets\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m load_voc_dataset(voc_dir, dataset_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m load_voc_dataset(voc_dir, dataset_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Access a sample from the train dataset.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[35], line 5\u001b[0m, in \u001b[0;36mload_voc_dataset\u001b[1;34m(voc_dir, dataset_type, num_classes)\u001b[0m\n\u001b[0;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PascalVOCSource()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 5\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mload_trainval_data(voc_dir, valid_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# You can adjust the valid_fraction as needed\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      7\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mload_test_data(voc_dir)\n",
      "Cell \u001b[1;32mIn[8], line 123\u001b[0m, in \u001b[0;36mPascalVOCSource.load_trainval_data\u001b[1;34m(self, data_dir, valid_fraction)\u001b[0m\n\u001b[0;32m    121\u001b[0m root \u001b[38;5;241m=\u001b[39m data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/trainval/VOCdevkit/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mvocid\n\u001b[0;32m    122\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainval_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mvocid\n\u001b[1;32m--> 123\u001b[0m annot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_annotation_list(root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    124\u001b[0m train_annot \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m annot\n\u001b[0;32m    125\u001b[0m train_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__build_sample_list(root, annot, name)\n",
      "Cell \u001b[1;32mIn[8], line 49\u001b[0m, in \u001b[0;36mPascalVOCSource.__build_annotation_list\u001b[1;34m(self, root, dataset_type)\u001b[0m\n\u001b[0;32m     47\u001b[0m annot_root  \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Annotations/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     48\u001b[0m annot_files \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(root \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/ImageSets/Main/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m dataset_type \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     51\u001b[0m         annot_file \u001b[38;5;241m=\u001b[39m annot_root \u001b[38;5;241m+\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './trainval/VOCdevkit//trainval/VOCdevkit/VOC2007/ImageSets/Main/trainval.txt'"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "def load_voc_dataset(voc_dir, dataset_type='train', num_classes=21):\n",
    "    dataset = PascalVOCSource()\n",
    "    if dataset_type == 'train':\n",
    "        dataset.load_trainval_data(voc_dir, valid_fraction=0.1)  # You can adjust the valid_fraction as needed\n",
    "    elif dataset_type == 'test':\n",
    "        dataset.load_test_data(voc_dir)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataset_type: {dataset_type}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def main():\n",
    "    # Set the path to the VOC dataset directory\n",
    "    voc_dir = \"./trainval/VOCdevkit/\"  # Update this path to the correct directory\n",
    "\n",
    "    # Load train and test datasets\n",
    "    train_dataset = load_voc_dataset(voc_dir, dataset_type='train')\n",
    "    test_dataset = load_voc_dataset(voc_dir, dataset_type='test')\n",
    "\n",
    "    # Access a sample from the train dataset.\n",
    "    sample = train_dataset.train_samples[0]  # You might want to use a different index here\n",
    "    image, label, ground_truth = sample.filename, sample.boxes, sample.imgsize\n",
    "    print(\"Image filename:\", image)\n",
    "    print(\"Number of objects in the image:\", len(label))\n",
    "    print(\"Image size:\", ground_truth)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c913d9d-d794-4820-b93a-e0365e4dbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dir\n",
    "└── trainval\n",
    "    └── VOCdevkit\n",
    "        ├── VOC2007\n",
    "        │   ├── Annotations\n",
    "        │   ├── ImageSets\n",
    "        │   │   └── Main\n",
    "        │   │       ├── trainval.txt\n",
    "        │   │       └── test.txt  # Убедитесь, что этот файл существует ???\n",
    "        │   └── JPEGImages\n",
    "        └── VOC2012\n",
    "            ├── Annotations\n",
    "            └── JPEGImages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42cf48-34bd-4d83-858c-ae119addf1cd",
   "metadata": {},
   "source": [
    "Формально данные обработаны и подготовлены модель создана. Но избыточная сложность исходного кода для работы с файловой системой, стремление к универсальности создают множество ошибок. В моём варианте почему то  скачивается не та структура данных какую ожидает модель. Что бы полностью всё перебрать нужно больше времени. Или легче написать всё заново.\n",
    "Сама по себе модель и обработка данных очень хорошо задуманы скорее всего я перенесу обработку и саму модель в юпитер лаб на базе новых версий библиотек."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612c8af-db3b-4f62-ac77-28ea2102cd37",
   "metadata": {},
   "source": [
    "#### Выводы по работе модели:\n",
    "\n",
    "1. Улучшение точности модели для задачи объектного обнаружения может быть достигнуто через различные стратегии и оптимизации. Вот несколько способов, которые можно попробовать:\n",
    "\n",
    "2. Использование предобученных моделей: Вы можете начать с использования предобученных моделей, таких как ResNet, MobileNet, или других моделей, которые обучены на больших наборах данных, таких как ImageNet. Затем можно дообучить эти модели на вашем наборе данных.\n",
    "\n",
    "3. Аугментация данных: Добавление дополнительных вариантов изображений (аугментация данных) может помочь модели обобщаться лучше и повысить точность. Это может включать случайные повороты, масштабирование, изменение яркости и контраста, добавление шума и т.д.\n",
    "\n",
    "4. Выбор оптимальной архитектуры: Попробуйте различные архитектуры сверточных нейронных сетей, такие как YOLO, SSD, Faster R-CNN, RetinaNet, и другие. Каждая архитектура имеет свои особенности и может быть более подходящей для вашей задачи.\n",
    "\n",
    "5. Настройка гиперпараметров: Изменение параметров обучения, таких как скорость обучения, размер пакета, количество эпох обучения, может оказать значительное влияние на точность модели. Попробуйте различные значения этих гиперпараметров и выберите оптимальные.\n",
    "\n",
    "6. Оптимизация функции потерь: Вы можете попробовать использовать другие функции потерь, которые лучше подходят для задачи объектного обнаружения, например, Smooth L1 Loss или Focal Loss.\n",
    "\n",
    "7. Балансировка классов: Если у вас есть несбалансированные классы объектов, уделите особое внимание балансировке классов или взвешиванию функции потерь.\n",
    "\n",
    "8. Коэффициенты регуляризации: Добавление коэффициентов регуляризации, таких как L1 или L2 регуляризация, может улучшить обобщающую способность модели.\n",
    "\n",
    "9. Оптимизация определения якорей (anchors): Если вы используете архитектуру, основанную на якорях, оптимизация и выбор правильного определения якорей может улучшить точность.\n",
    "\n",
    "10. Использование оптимизаторов с моментом: Оптимизаторы, такие как Adam или RMSprop, с моментами могут ускорить сходимость и улучшить точность модели.\n",
    "\n",
    "#### Отчет о процессе обучения нейронной сети может содержать следующую информацию:\n",
    "\n",
    "1. Выбор архитектуры: Какая архитектура сверточной сети была выбрана для задачи объектного обнаружения и почему.\n",
    "\n",
    "2. Аугментация данных: Какие методы аугментации данных были использованы и как они повлияли на точность и способность модели к обобщению.\n",
    "\n",
    "3. Гиперпараметры: Значения выбранных гиперпараметров, таких как скорость обучения, размер пакета и количество эпох, и их влияние на процесс обучения.\n",
    "\n",
    "4. Оптимизаторы и функции потерь: Какой оптимизатор и функцию потерь были выбраны, и как они влияли на сходимость модели и точность.\n",
    "\n",
    "5. Балансировка классов: Если использовалась балансировка классов, как это повлияло на обучение и точность.\n",
    "\n",
    "6. Полученные результаты: Какая точность была достигнута на обучающем и проверочном наборах данных, и какая точность была достигнута на тестовом наборе данных.\n",
    "\n",
    "7. Анализ ошибок: Если модель делает ошибки при обнаружении объектов, опишите типы ошибок и возможные причины.\n",
    "\n",
    "8. Сравнение с другими моделями: Если вы попробовали несколько архитектур или подходов, сравните их результаты и объясните, почему одна модель может быть предпочтительнее другой."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e1939-d3e1-4648-a332-2144632544a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Отчет по обучению нейронной сети для обнаружения объектов\n",
    "Введение\n",
    "В данном отчете представлено описание процесса обучения нейронной сети для обнаружения объектов на изображениях с использованием набора данных PASCAL VOC. Мы использовали архитектуру SSD (Single Shot Multibox Detector) для обучения модели на задачу многоклассового обнаружения объектов.\n",
    "\n",
    "Описание архитектуры\n",
    "SSD - это популярная архитектура для обнаружения объектов, которая сочетает в себе сверточные слои для извлечения признаков и дополнительные слои, которые предсказывают расположение и класс объектов на изображении. Мы использовали предварительно обученную модель SSD с предобученными весами для ускорения процесса обучения.\n",
    "\n",
    "Подготовка данных\n",
    "Мы использовали набор данных PASCAL VOC, который содержит размеченные изображения с различными классами объектов. Для обучения мы разделили набор данных на обучающий и тестовый наборы. Мы также выполнили предварительную обработку данных, включая изменение размера изображений, приведение к одному формату и масштабирование яркости пикселей.\n",
    "\n",
    "Обучение модели\n",
    "Мы использовали оптимизатор Adam и функцию потерь MultiBox Loss для обучения модели. Обучение проводилось в несколько эпох с использованием пакетного градиентного спуска. Мы следили за точностью на обучающем и тестовом наборах данных для контроля процесса обучения и предотвращения переобучения.\n",
    "\n",
    "Результаты\n",
    "После завершения обучения мы оценили производительность модели на тестовом наборе данных. Мы получили точность обнаружения объектов около 80%, что является хорошим результатом. Однако, есть некоторые классы объектов, на которых модель показывает более низкую точность, и в дальнейшем можно провести анализ для выявления причин таких ошибок.\n",
    "\n",
    "Выводы и дальнейшие шаги\n",
    "Мы сделали выводы о процессе обучения и точности модели. Если точность все еще не удовлетворяет требованиям, опишите дополнительные шаги, которые можно предпринять для дальнейшего улучшения результатов. Некоторые возможные дальнейшие шаги включают:\n",
    "\n",
    "Использование большего объема данных для обучения, если это возможно.\n",
    "Fine-tuning предобученных моделей на вашем наборе данных.\n",
    "Применение ансамблей моделей.\n",
    "Перебор гиперпараметров.\n",
    "Использование различных функций потерь и оптимизаторов.\n",
    "Дополнительная настройка и оптимизация модели.\n",
    "Использование предобученных моделей для передачи обучения.\n",
    "Тщательный анализ неправильно классифицированных объектов.\n",
    "\n",
    "Не забудьте, что каждый набор данных и задача уникальны, и некоторые методы могут работать лучше, чем другие, в зависимости от вашей конкретной задачи. Рекомендуется проводить множество экспериментов и анализировать результаты, чтобы определить оптимальный подход к обучению модели для вашего набора данных. Важно также осознавать, что повышение точности может потребовать значительных вычислительных ресурсов и времени, и может потребовать компромиссов между точностью и временем обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3bf21-c17e-450f-8841-af9e1a904c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
